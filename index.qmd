---
title: Nonlinear dynamics with neural networks
subtitle: Preliminary Examination
author: Anil Radhakrishnan
institute: Nonlinear Artificial Intelligence Laboratory, North Carolina State University
date: last-modified
bibliography: references.bib
csl: nature.csl

format:
  revealjs:
    theme: [default, assets/custom.scss]
    logo: assets/NAIL_state.png
    css: assets/style.css
    slide-number: true
    toc: true
    toc-depth: 1
    show-slide-number: all
    chalkboard: true
    progress: true
    touch: true
    keyboard: true
    mouse-wheel: true
    controls: auto
    auto-play-media: true
    citations-hover: true
    reference-location: section
    include-in-header: 
      text: |
        <style>
        .center-xy {
          margin: 0;
          position: absolute;
          top: 0%;
          -ms-transform: translateY(20%);
          transform: translateY(20%);
        }
        </style>
revealjs-plugins:
  - attribution
    
---
# Motivation
## Motivation: Why Neural Networks?
- The preeminent computational tool of the contemporary world.
- Differentable, optimizable, and scalable.
- Emergent uses in and out of Physics @vicentini_netket_2022.

::: {layout-ncol=3}

![](assets/ChatGPT.png){fig-alt="ChatGPT" fig-align="left" width="250"}

![](assets/Microsoft_365_Copilot_Icon.png){fig-alt="CoPilot" fig-align="center" width="250"}

![](assets/netket.png){fig-alt="NetKet" fig-align="right" width="250"}

:::

::: {.notes}
Speaker notes go here.
:::
  
## Motivation: Why Nonlinear Dynamics?
- Captures complexity.
- Formal, well established framework.
- Emergent uses in and out of Physics.

<div style="padding:50% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/739921904?h=f803d0bbff&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:5%;width:90%;height:90%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Animation by [Wliiam Gilpin](https://github.com/williamgilpin/cphy)
:::

::: {.notes}
Avalanche activity cascades in a sandpile automaton; a vortex street formed by flow past a cylinder; and Turing patterns in a reaction-diffusion model.
:::

## Motivation: Why Nonlinear Dynamics for Neural Networks?
- Optimization is inescapably nonlinear @sohl-dickstein_boundary_2024.
- Neural networks are inherently nonlinear.
- Scope of nonlinearities in neural networks is underexplored.
  
<div style="padding:90% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/903855670?h=ca2b077023&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:30%;width:40%;height:40%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Trainability fractal by [Jascha Sohl-Dickstein](https://sohl-dickstein.github.io/2024/02/12/fractal.html)
:::
::: {.notes}
Each pixel corresponds to training the same neural network from the same initialization on the same data — but with different hyperparameters. Blue-green colors mean that training converged for those hyperparameters, and the network successfully trained. Red-yellow colors mean that training diverged for those hyperparameters. The paler the color the faster the convergence or divergence

The neural network consists of an input layer, a tanh nonlinearity, and an output layer. In the image, the x-coordinate changes the learning rate for the input layer’s parameters, and the y-coordinate changes the learning rate for the output layer’s parameters.
:::

## Motivation: Why Neural Networks for Nonlinear Dynamics?
- Nonlinear dynamical systems are computationally expensive to solve.
- Paradigm of solution as an element of a distribution translates naturally to neural networks.
- Data-driven methods accommodate realistic complexity.

<!-- <img src="assets/escher_day_night.png" alt="Escher's day and night wood blood painting" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">

  
::: {.attribution}
Day and Night by [M.C. Escher](https://www.wikiart.org/en/m-c-escher/day-and-night)
::: -->

# Neural Networks and optimization

## Background: Differentiable Computing

- Paradigm where programs can be differentiated end-to-end automatically, enabling optimization of parameters of the program @blondel_elements_2024.
- Techniques to differentiate through complex programs is more than just deep learning.
- Can be interpreted probabilistically or as a dynamical system.
![](figures/BG1/autodiff.png){fig-alt="autodifferentiation diagram" width=80% }


## Background: Neural Networks

![](figures/BG1/NN.png){fig-alt="Neural Network diagram" width=80%}
<div style="margin-top: -20px;">
$$
\newcommand{\defeq}{\mathrel{\mathop:}=}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Param}{\theta}
\newcommand{\Loss}{\mathcal{L}}
$$

$$\varphi: \R^d \rightarrow \R^N_L \qquad T^l: \R^{N_{l-1}}\rightarrow \R^{N_{l}} \qquad \sigma: \R\rightarrow \R$$
$$x \in \R^d \qquad W^l \in \R^{N_l\times N_{l-1}} \qquad b^l \in \R^{N_l}$$

## Background: Backpropagation

Consider a differentiable loss function $\Loss$,

:::: {layout="[ 40, 60 ]"}

::: {#first-column}
$$
\begin{eqnarray*}
\delta^L =& \nabla_{\varphi}\Loss\odot\sigma'(T^L)\\
\delta^l =& ((w^{l+1})^\intercal \delta^{l+1} ))\odot\sigma'(T^l)\\
\dfrac{\partial \Loss}{\partial b^l_j} =& \delta_j^l \\
\dfrac{\partial \Loss}{\partial w^l_{jk}} =& T^{l-1}_k\delta_j^l \\
\end{eqnarray*}
$$
:::

::: {#second-column}
![](figures/BG1/backprop_zoomed.png){fig-alt="backprop diagram" fig-align="right" width=80%}
:::
::::
The tunable parameters can then be updated by:
$\Param\gets \Param -\eta \nabla_{\Param}\Loss$

## Background: Loss Functions
- Neural networks compute a probability distribution on the data space. 
- Our objective is maximize the likihood the assigned to emperical data.
- Constructing a suitable differentiable loss function gives the path to optimization of a neural network.
Common loss functions include:
  - Mean Squared Error $\Loss(\theta) = \sum_{i=1}^N (y_i - f(x_i;\theta))^2$
  - Cross Entropy $\Loss(\theta) = -\sum_{i=1}^N y_i \log(f(x_i;\theta))$
  - Kullback-Leibler Divergence $\Loss(\theta) = \sum_{i=1}^N y_i \log\left(\dfrac{y_i}{f(x_i;\theta)}\right)$
  
Loss functions are combined and regularized to balance the tradeoff between model complexity and data fit.

## Background: Optimization
- To find the best model parametrization, we minimize the loss function with respect to the model parameters. 
- That is we compute, $\mathcal{L^\star} \defeq \inf_{\theta \in \Theta} \Loss(\theta)$ assuming an infimum exists.
- To converge to a minima the optimizer needs an oracle $\mathcal{O}$, i.e. evaluation of the Loss function, its gradients, or higher order derivatives.
Then, for an algortihm $\mathcal{A}$,
$$
  \theta ^ {t+1} 
  \defeq \mathcal{A}(\theta ^ 0, \ldots, \theta ^ t, 
    \mathcal{O}(\theta ^ 0), \ldots, \mathcal{O}(\theta ^ t), \lambda),
$$
where $\lambda \in \Lambda$ is a hyperparameter.

Stochastic Gradient Descent, Adam, and RMSProp are common optimization algorithms for training neural networks.

## Background: Meta-Learning

:::: {layout="[ 50, 50 ]"}

::: {#first-column}
- Improve the learning algorithm itself given the experience of mutiple learning episodes.
- Base learning: an inner learning algorithm solves a task defined by a dataset and objective. 
- Meta-learning: an outer algorithm updates the inner learning algorithm.
:::

::: {#second-column}
![](figures/BG1/Metalearning.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::
Algorithms for meta-learning are still in a nascent stage with significant computational overhead.

::: {.attribution}
Figure by [John Lindner ](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Background: Physics-Informed Neural Networks(PiNNs)
- Synthesyzing data with differential equation constraints.
- Physics as a weak constraint in a composite loss function or a hard constraint with architectural choices.
- Symplectic constraints to the loss function gives Hamiltonian Neural Networks @greydanus_hamiltonian_2019.

<img src="figures/BG1/HNN.png" alt="HNN schema" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">


## Background: Coordinates matter

<!-- {{<video figures/coordinates.mp4 width="500" height="400" autoplay="true" loop="true">}} -->
<div>
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG1/coordinates.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>
Neural networks are coordinate dependent. The choice of coordinates can significantly affect the performance of the network.

::: {.attribution}
Adapted from Heliocentrism and Geocentrism by [Malin Christersson
](https://www.malinc.se/math/)
:::


# Metalearning Activation functions 
(Published, US and International Patent Pending)
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script><div class="altmetric-embed" data-badge-type="large-donut" data-altmetric-id="126463806"></div>

## Foundation

- Most complex systems showcase diversity in populations.
- Artificial neural network architectures are traditionally layerwise-homogenous
- Will neurons diversify if given the opportunity?

## Insight
:::: {layout="[ 65, 35 ]"}

::: {#first-column}
::: {.center-xy}
- Activation functions can themselves be modeled as neural networks.
- Activation function subnetworks are optimizable via metalearning.
- Multiple subnetwork initializations allow the activations to _diversify_ and evolve into different communities.
:::
:::

::: {#second-column}
![](figures/Result1/meta_act_schema_vert.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Methodology
:::: {layout="[ 60, 40 ]"}

::: {#first-column}

- Developed a metalearning framework to optimize activation functions.
- Tested the algorithm on classification and regression tasks for conventional and physics-informed neural networks.
- Showed a regime where learned diverse activations are superior.
- Gave preliminary analysis to support diversity in activation functions improving performance.

:::

::: {#second-column}

![](figures/Result1/2NeuronSubpopulation_Sine_1_compact2.png){fig-alt="2neuronmnist1d" fig-align="center" width=100%}

:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Results: Scaling
![](figures/Result1/meta_div_scaling.png){fig-alt="scaling" fig-align="center" width=80%}


::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Results: Real World Data
![](figures/Result1/real_pendulum.png){fig-alt="real world example" fig-align="center" width=80%}

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Analysis: Participation Ratio
Estimate change in dimensionality of network activations

:::: {layout="[ 65, 35 ]"}

::: {#first-column}

$$
Nr = \mathcal{R} 
    = \frac{(\operatorname{tr}{\bf C})^2}{\operatorname{tr}{\bf C}^2}
    = \frac{\left(\sum_{n=1}^N\lambda_n \right)^2}{\sum_{n=1}^N \lambda_n^2}
$$

where $\lambda_n$ are the co-variance matrix $\bf C$'s eigenvalues for neuronal activity data matrix.
The normalized participation ratio $r = \mathcal{R} / N$ @gao_theory_2017.

:::

::: {#second-column}

![](figures/Result1/Participation_ratio.png){fig-alt="participation ratio" fig-align="center" width=100%}

:::
::::

Diverse activation functions use more of the network's capacity.
<!-- The participation ratio is significantly greater for the diverse activation functions indicating a greater use of the network's capacity. -->

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

::: {.notes}
If all the variance is in one dimension, say $\lambda_n = \delta_{n1}$, then $\mathcal{R} = 1$

If the variance is evenly distributed across all dimensions, so $\lambda_n = \lambda_1$, then $\mathcal{R} = N$. 

Typically, $1 < \mathcal{R} < N$, and $\mathcal{R}$ corresponds to the number of dimensions needed to explain most of the variance
:::

## Conclusions

- Learned Diversity Networks discover sets of activation functions that can outperform traditional activations.
- These networks excel in the regime of low data few shot learning.
- Due to the nature of metalearning, the memory overhead is significant concern for scaling.

## Speculations: Achieving stable minima

<div style="margin-bottom: 30px;">
![](figures/Result1/Noisy_descent.png){fig-alt="Noisy descent" fig-align="center" width=30%}
</div>

&nbsp;

Optimization of a neural network with shuffled data is a noisy descent.

This can be modeled with the Langevin equation:

$$
\DeclareMathOperator{\Tr}{Tr}
d\theta_{t} = - \nabla \Loss(\theta_{t})\, dt + \sqrt{2\mathbf{D}} \cdot d\mathcal{W}_{t}
$$
with noise intensity $\mathbf{D} = (\eta / B) \Loss(\theta) \mathbf{H}(\theta\star)$ @mori_power-law_2022

<!-- Resulting minima from diverse neurons is flatter than from homogeneous ones, as measured by both the trace $\Tr\mathbf{H}$ of the Hessian and the fraction $f$ of its eigenvalues near zero: $\Tr \mathbf{H}_1 >  \Tr \mathbf{H}_2 > \Tr \mathbf{H}_{12}$ and $f_1 < f_2 < f_{12}$

Since the noise term aligns with Hessian bear the minima, this suggests that the minima found by diverse neurons are flatter and more stable. -->

## Speculations: Structure and universality to diverse activations

- Learned activation functions appear qualitatively independent of the base activation function.
- The odd and even nature of the learned functions suggest that the network is learning to span the space of possible activation functions.

![](figures/Result1/learned_act_insight.png){fig-alt="spanning sets" fig-align="center" width=80%}



# Control and Chaos

## Background: Hamilton Jacobi Bellman(HJB) Equation
Extension of Hamilton Jacobi equation/ Continous time analog of Dynamic Programming.

$$
\dfrac{dx}{dt} = f(x(t),u(t))dt \\
J(x,u,t_0,t_f) = Q(x(t_f), t_f) + \int_{t_0}^{t_f} \Loss(x(\tau),u(\tau))d\tau\\
V(x(t_0),t_0,t_f) = \min_{u(t)} J(x(t_0),u(t),t_0,t_f)\\
-\dfrac{\partial V}{\partial t} = \min_{u(t)} \left[ \Loss(x(t),u(t)) + \dfrac{\partial V}{\partial x}^T f(x(t),u(t)) \right]
$$

:::{.notes}
V(x,t) is the value function, the minimum cost-to-go from state x at time t.
If we can solve for V then we can find from it a control u that achieves the minimum cost.
:::

## Background: Model Predictive Control

Control scheme where a model is used for predicting the future behavior of the system over finite time window @fiedler_-mpc:_2023.

<table style="width:100%; margin:0; padding:0;">
  <tr>
    <td style="vertical-align:top; padding-right:20px; width:65%">
<video width="700" height="300" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/BG2/MPC.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<video width="200" height="200" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/disc_3d_ctrl_motor.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
    <td style="vertical-align:top; width:35%">
![](figures/BG2/MPC_basic.png){fig-alt="Model Predictive control" fig-align="center" width=100%}
    </td>
  </tr>
</table>

::: {.attribution}
Animations from [do-mpc documentation](https://www.do-mpc.com/en/latest/theory_mpc.html)
:::

::: {.notes}
Based on these predictions and the current measured/estimated state of the system, the optimal control inputs with respect to a defined control objective and subject to system constraints is computed. 
After a certain time interval, the measurement, estimation and computation process is repeated with a shifted horizon

Proactive control action: The controller is anticipating future disturbances, set-points etc.

Non-linear control: MPC can explicitly consider non-linear systems without linearization

Arbitrary control objective: Traditional set-point tracking and regulation or economic MPC

constrained formulation: Explicitly consider physical, safety or operational system constraints
The dotted line indicates the current prediction and the solid line represents the realized values.
:::

## Background: Chaos

Let $X$ be a metric space. A continous map $f: X \rightarrow X$ is said to be chaotic on $X$ if:

<table style="width:100%">
  <tr>
    <td style="vertical-align:top">

- $f$ has sensitive dependence on initial conditions
- is topologically transitive
- and has dense periodic orbits @devaney_introduction_2003

    </td>
    <td style="vertical-align:top">
<video width="600" height="700" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/lorentz_attractor.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
  </tr>
</table>

## Background: Traditional Chaos Control

Relies on the ergodicity of chaotic systems [@ott_controlling_1990; @ditto_experimental_1990].

![](figures/BG2/OGY.png){fig-alt="Ott-Gerbogi-Yorke control" fig-align="center" width=80%}

![](figures/BG2/UPO.png){fig-alt="Unstable periodic orbits" fig-align="center" width=40%}



## Background: Chaotic Pendulum Array @braiman_taming_1995
<div>
<video width="700" height="400" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/Chaotic_pendulum_array.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>

$$
l_n^2\ddot\theta_n =-\gamma\dot\theta_n\nonumber-l_n\sin{\theta_n}\nonumber +\tau_{0}+\tau_{1}\sin{\omega t}\nonumber +\kappa(\theta_{n-1}+\theta_{n+1}-2\theta_n)
$$

## Background: Kuramoto Oscillator @kuramoto_international_1975

<div>
<table style="width:100%; margin:0; padding:0;">
  <tr>
    <td style="vertical-align:top; padding-right:20px; width:65%">
<video width="300" height="600" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/BG2/fireflies.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
    <td style="vertical-align:top; width:35%">
<video width="700" height="300" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/firefly_vid.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
$$\dot{\theta}_i = \omega_i + \lambda\sum_{j=1}^N \sin(\theta_j - \theta_i)$$
    </td>
  </tr>
</table>
</div>


:::{.attribution}
Synchronizing fireflies video from [Robin Meier](http://robinmeier.net/?p=2279)
:::

# Neural Network control of chaos

## Insight
- Optimal control of network dynamics involves minimizing a cost functional.
- Traditional control approaches like Pontryagin's maximum principle or Hamilton-Jacobi-Bellman equations are analytically and computationally intractable for complex systems.
- Neural networks based on neural ODES can approximate the optimal control policy @bottcher_ai_2022.

## Methodology

![](figures/Result2/MPC_NN.png){fig-alt="Model Predictive control" fig-align="center" width=100%}

## Results: Kuramoto Grid

![](figures/Result2/kuramoto_control.png){fig-alt="Controlled kuramoto dynamics" fig-align="center" width=100%}

## Results: Pendulum Array

![](figures/Result2/controlled_pendulum_dynamics.png){fig-alt="Controlled Pendulum dynamics" fig-align="center" width=70%}

<div style="margin-top: 30px;">
<video width="700" height="400" style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/Result2/Pendulum_optimal.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>

## Future Work: Exotic Dynamics @abrams_chimera_2004
::: {style="font-size: 70%;"}
$$\dot{\theta}_i = \omega_i + \lambda\sum_{j=1}^N A_{ij}\sin(\theta_j - \theta_i-\alpha)$$
:::
<div>
<table style="width:100%; margin:0; padding:0; margin-top:-65px;">
  <tr>
    <td style="vertical-align:top; padding-right:20px; width:50%">
<video width="300" height="600" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/Result2/ks_basic.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
    <td style="vertical-align:top; padding-right:20px; width:50%">
<video width="300" height="600" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/Result2/ks_chimera.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
  </tr>
</table>
</div>

## Future Work: Recurrence matrices in $\mathcal{L}$ @kennel_determining_1992

![](figures/Result2/recurrence_matrix_lorenz.png){fig-alt="Recurrence matrix for lorenz attractor" fig-align="center" width=70%}

# Dynamics, symmetries, and introspection
## Background: Group theory for Dynamical Systems

Let $\Gamma$ act on $\mathbb{R}^n$ and $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ . Then $f$ is said to be $\Gamma$-equivariant if $f(\gamma x) = \gamma f(x)$ for all $x \in \mathbb{R}^n$ and $\gamma \in \Gamma$ @zee_group_2016.

For dynamical systems, for a fixed point $f(x\star)=0$, $\gamma x\star$ is also a fixed point.

Isotropy subgroup:
Let $v\in \mathbb{R}^n: \Sigma_v = \{\gamma \in \Gamma: \gamma v = v\}$

Fixed pt subspace:
Let $\Sigma \subseteq \Gamma$. $\text{Fix}(\Sigma)=\{v \in \mathbb{R}^n:\sigma v=v\}$

*Thm*: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a $\Gamma$-equivariant map and  $\Sigma \subseteq \Gamma$. Then $f(\text{Fix}(\Sigma))\subseteq\text{Fix}(\Sigma)$.

:::{.notes}
Cyclic group
Dihedral group = Cyclic group + Flips
Symmetric group = m!
S^1 circle group
O(n) Measure preserving transformations

For ODEs that means if $f(x)$ is a solution, then so is $f(\gamma x)$ for all $gamma \in \Gamma$.
So we can classify solutions upto their symmetry.

If a system starts in a symmetric state, it will remain symmetric.
:::

## Background: Group Equivariant Neural Networks

<div style="margin-bottom: 40px; text-align: center;">
<img src="figures/BG3/lift_project.png" alt="Group equivariance works by lifting and projection" style="width: 50%;">
</div>

Group equivariant NN work through lifting convolutions to the _group_ space, and then projecting back to the original space @cohen_group_2016.

:::{.attribution}
Figure adapted from [Bart Smets et al.](https://arxiv.org/abs/2001.09046)
:::

# Symmetries of controllers

## Insight

- Dynamical systems often obey symmetries.
- Controllers are essentially dynamical systems.
- Is there a computationally viable mapping between the symmetries of the system and the controller?

## Methodology

- Analyze the controllers from the previous sections for symmetries using group equivariant autoencoders
- Construct controllers that respect the symmetries of the system
- Compare the performance of symmetric to conventional controllers.

## Expectation

- Viability test for group equivariant neural networks in control systems.
- Mapping between the symmetries of the system and the controller.
- Performance analysis of symmetric controllers.

# Conclusion
- Non-traditional treatments of neural networks let us better capture nonlinearity.
- Standard paradigms of Geometry, Statistics, and Algebra for understanding nonlinear systems are augmented by neural networks.
- The interplay of Physics, Mathematics, and Computer Science gives us the best shot at understanding complex systems

## Deliverables
- Publishable article on Neural Network Control of Chaos.
- Thorough study of the symmetries of controllers.
- Codebase for the above studies.
- Dissertation detailing the discoveries and pitfalls found throughout.

Estimated time of completion: August 2025

## Acknowledgements {background-image="assets/ks_pretty.png" background-size="contain" background-position="center" background-repeat="no-repeat}

<!-- ![](assets/acknowledgements.png){fig-alt="Bill, John, Anshul" fig-align="center" width="250"}

![](assets/Microsoft_365_Copilot_Icon.png){fig-alt="CoPilot" fig-align="center" width="250"} -->

<div style="text-align: center; margin-top: 100px;">
<img src="assets/acknowledgements.png" alt="Bill, John, Anshul" style="width: 400px;">
</div>

:::{.attribution}
Background image from [Jacqueline Doan](https://community.wolfram.com/groups/-/m/t/2509110)
:::

## References

---
nocite: |
  @*
---

::: {#refs}
:::



# Backup slides {.unnumbered .unlisted visibility="uncounted"}

## Universal approximation theorem {.unnumbered .unlisted visibility="uncounted"}

$$M(\sigma) = \text{span}\{\sigma(wx-\theta):w\in \mathbb{R}^n, \theta\in\mathbb{R}\}$$
<!-- For which $\sigma$ is $\max_{x\in K}|f(x)-g(x)|<\epsilon$ true for any $f\in C(\mathbb{R}^n)$, any compact subset $K$ of $\mathbb{R}^n$, and $\epsilon >0$, and some $g \in M(\sigma)$? -->

*Theorem 1*: Let $\sigma \in C(\mathbb{R})$. Then $M(\sigma)$ is dense in $C(\mathbb{R}^n)$, in the topology of uniform convergence on compacta, if and only if $\sigma$ is not a polynomial @leshno_multilayer_1993.

For Neural Networks,

*Theorem 2*: Let $\sigma$ be any continous sigmoidal function. Then finite sums of the form $G(x) = \sum_{i=1}^N \alpha_i \sigma(w_i^T x + b_i)$ are dense in $C(I_n)$.
i.e. $\forall f\in C(I_n)$ and $\epsilon>0$, there is a sum $G(x)$ such that $\max_{x\in I_n}|f(x)-G(x)|<\epsilon$ @cybenko_approximation_1989.

Informally, at least one neural network exists that can approximate any continuous function on $I_n=[0,1]^n$ with arbitrary precision.

## SGD via gradient flows {.unnumbered .unlisted visibility="uncounted" .smaller}

::: {style="font-size: 78%;"}

Gradient descent: $x_{n+1} = x_n - \gamma\nabla f(x_n)$ where $\gamma > 0$ is the step-size. Gradient flow is the limit where $\gamma\rightarrow 0$.

There are two ways to treat SGD in this regime,

Consider fixed times $t=n\gamma$ and $s=m\gamma$.

:::: {layout="[ 45, 55 ]"}

::: {#first-column}

- Convergence to gradient flow
  
Given the recursion $x_{n+1} = x_n-\gamma\nabla f(x_n) - \gamma\epsilon_n$,

applying this $m$ times, we get:

$$
\begin{align*}
X(t+s) - X(t) &= x_{n+m}-x_n\\
              &= -\gamma \sum_{k=0}^{m-1}\nabla f(X(t+\frac{sk}{m})) - \gamma\sum_{k=0}^{m-1}ε_{k+n}
\end{align*}
$$
in the limit,
$$
\begin{align*}
X(t+s) - X(t) &= -\int_t^{t+s}\nabla f(X(u))du + 0
\end{align*}
$$
which is just the gradient flow equation.

:::

::: {#second-column}

- Convergence to Langevin diffusion

Given the recursion $x_{n+1} = x_n-\gamma\nabla f(x_n) - \sqrt{2\gamma}\epsilon_n$,

applying this $m$ times, we get:

$$
\begin{align*}
X(t+s) - X(t) &= x_{n+m}-x_n\\
              &= -\gamma \sum_{k=0}^{m-1}\nabla f(X(t+\frac{sk}{m}))- \sqrt{2\gamma}\sum_{k=0}^{m-1}ε_{k+n}
\end{align*}
$$
The second term has finite variance $\propto 2s$. When $m$ tends to infinity,

$$
X(t+s) - X(t) =\\
 -\int_t^{t+s} \nabla f(X(u))du + \sqrt{2}[B(t+s)-B(t)]
$$

This limiting distribution is the Gibbs distribution with density $\exp{(-f(x))}$

:::
::: 

:::

:::{.notes}
γ∑m−1k=0εk+n
 has zero expectation and variance equal to γ2m=γs
 times the variance of each εk+n and thus tends to 0
:::

:::{.attribution}
Argument from [Francis Bach](https://francisbach.com/gradient-flows/)
:::

## Neural ODEs {.unnumbered .unlisted visibility="uncounted"}

## HJB Derivation Sketch {.unnumbered .unlisted visibility="uncounted"}
Bellman optimality equation:
$$
V(x(t_0),t_0,t_f) = V(x(t_0),t_0,t) + V(x(t),t,t_f)
$$
$$
\begin{align*}
  \dfrac{dV(x(t),t,t_f)}{dt} &= \dfrac{\partial V}{\partial t} + \dfrac{\partial V}{\partial x}^T \frac{dx}{dt}\\
  &= \min_{u(t)} \dfrac{d}{dt} \left[ \int_0^{t_f}\Loss(x(\tau),u(\tau))d\tau + Q(x(t_f), t_f) \right]\\
  &= \min_{u(t)} \left[ \dfrac{d}{dt}\int_0^{t_f}\Loss(x(\tau),u(\tau))d\tau \right] \\
\end{align*}
$$

$$
\implies \boxed{-\dfrac{\partial V}{\partial t} = \min_{u(t)} \left[ \Loss(x(t),u(t)) + \dfrac{\partial V}{\partial x}^T f(x(t),u(t)) \right]}
$$

## Pontryagin's Maximum Principle {.unnumbered .unlisted visibility="uncounted"}

## Stochasticity and noise {.unnumbered .unlisted visibility="uncounted"}

## Delay coordinates and noise {.unnumbered .unlisted visibility="uncounted"}

## Neural network architectures {.unnumbered .unlisted visibility="uncounted"}

