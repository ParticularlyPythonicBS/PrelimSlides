---
title: Nonlinear dynamics with neural networks
subtitle: Preliminary Examination
author: Anil Radhakrishnan
institute: Nonlinear Artificial Intelligence laboratory, North Carolina State University
date: last-modified
format:
  revealjs:
    theme: [default, assets/custom.scss]
    logo: assets/NAIL_state.png
    css: assets/style.css
    slide-number: true
    toc: true
    toc-depth: 1
    show-slide-number: all
    chalkboard: true
    progress: true
    touch: true
    keyboard: true
    mouse-wheel: true
    controls: auto
    auto-play-media: true
    citations-hover: true
    reference-location: section
    # include-in-header: 
    #   text: |
    #     <style>
    #     .center-xy {
    #       margin: 0;
    #       position: absolute;
    #       top: 0%;
    #       -ms-transform: translateY(20%);
    #       transform: translateY(20%);
    #     }
    #     </style>
revealjs-plugins:
  - attribution
    
---
# Motivation
## Motivation: Why Neural Networks?
- The preeminent computational tool of the contemporary world.
- Differentable, optimizable, and scalable.
- Emergent uses in and out of Physics.

::: {layout-ncol=3}

![](assets/ChatGPT.png){fig-alt="ChatGPT" fig-align="left" width="250"}

![](assets/Microsoft_365_Copilot_Icon.png){fig-alt="CoPilot" fig-align="center" width="250"}

![](assets/netket.png){fig-alt="NetKet" fig-align="right" width="250"}

:::

::: {.notes}
Speaker notes go here.
:::
  
## Motivation: Why Nonlinear Dynamics?
- Captures complexity.
- Formal, well established framework.
- Emergent uses in and out of Physics.

<div style="padding:50% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/739921904?h=f803d0bbff&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:5%;width:90%;height:90%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Animation by [Wliiam Gilpin](https://github.com/williamgilpin/cphy)
:::

::: {.notes}
Avalanche activity cascades in a sandpile automaton; a vortex street formed by flow past a cylinder; and Turing patterns in a reaction-diffusion model.
:::

## Motivation: Why Nonlinear Dynamics for Neural Networks?
- Optimization is inescapably nonlinear.
- Neural networks are inherently nonlinear.
- Scope of nonlinearities in neural networks is underexplored.
  
<div style="padding:90% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/903855670?h=ca2b077023&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:30%;width:40%;height:40%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Trainability fractal by [Jascha Sohl-Dickstein](https://sohl-dickstein.github.io/2024/02/12/fractal.html)
:::
::: {.notes}
Each pixel corresponds to training the same neural network from the same initialization on the same data — but with different hyperparameters. Blue-green colors mean that training converged for those hyperparameters, and the network successfully trained. Red-yellow colors mean that training diverged for those hyperparameters. The paler the color the faster the convergence or divergence

The neural network consists of an input layer, a tanh nonlinearity, and an output layer. In the image, the x-coordinate changes the learning rate for the input layer’s parameters, and the y-coordinate changes the learning rate for the output layer’s parameters.
:::

## Motivation: Why Neural Networks for Nonlinear Dynamics?
- Nonlinear dynamical systems are computationally expensive to solve.
- Paradigm of solution as an element of a distribution translates naturally to neural networks.
- Data-driven methods accommodate realistic complexity.

<img src="assets/escher_day_night.png" alt="Escher's day and night wood blood painting" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">

  
::: {.attribution}
Day and Night by [M.C. Escher](https://www.wikiart.org/en/m-c-escher/day-and-night)
:::

# Neural Networks and optimization

<!-- ![](assets/machine_learning_xkcd.png){fig-alt="Machine Learning XKCD" fig-align="center" height="350px"}

::: {.attribution}
XKCD comic by [Randall Munroe](https://xkcd.com/1838/)
::: -->

## Background: Differentiable Computing
- Paradigm where programs can be differentiated end-to-end automatically, enabling optimizatio of parameters of the program.
- Techniques to differentiate through complex programs is more than just deep learning.
- Can be interpreted probabilistically or as a dynamical system.
![](figures/BG1/autodiff.png){fig-alt="autodifferentiation diagram" width=80% }


## Background: Neural Networks

![](figures/BG1/NN.png){fig-alt="Neural Network diagram" width=80%}
$$
\newcommand{\defeq}{\mathrel{\mathop:}=}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Param}{\mathcal{P}}
$$

$$\varphi: \R^d \rightarrow \R^N_L \qquad T^l: \R^{N_{l-1}}\rightarrow \R^{N_{l}} \qquad \rho: \R\rightarrow \R$$
$$x \in \R^d \qquad W^l \in \R^{N_l\times N_{l-1}} \qquad b^l \in \R^{N_l}$$

## Background: Backpropagation

Consider a differentiable cost function $C$,

:::: {layout="[ 40, 60 ]"}

::: {#first-column}
$$
\begin{eqnarray*}
\delta^L =& \nabla_{\varphi}C\odot\rho'(T^L)\\
\delta^l =& ((w^{l+1})^\intercal \delta^{l+1} ))\odot\rho'(T^l)\\
\dfrac{\partial C}{\partial b^l_j} =& \delta_j^l \\
\dfrac{\partial C}{\partial w^l_{jk}} =& T^{l-1}_k\delta_j^l \\
\end{eqnarray*}
$$
:::

::: {#second-column}
![](figures/BG1/backprop_zoomed.png){fig-alt="backprop diagram" fig-align="right" width=80%}
:::
::::
The tunable parameters can then be updated by:
$\Param\gets \Param -\eta \nabla_{\Param}C$

## Background: Loss Functions
- Neural networks compute a probability distribution on the data space. 
- Our objective is maximize the likihood the assigned to emperical data.
- Constructing a suitable differentiable loss function gives the path to optimization of a neural network.
Common loss functions include:
  - Mean Squared Error $\mathcal{L}(\theta) = \sum_{i=1}^N (y_i - f(x_i;\theta))^2$
  - Cross Entropy $\mathcal{L}(\theta) = -\sum_{i=1}^N y_i \log(f(x_i;\theta))$
  - Kullback-Leibler Divergence $\mathcal{L}(\theta) = \sum_{i=1}^N y_i \log\left(\dfrac{y_i}{f(x_i;\theta)}\right)$
  
Loss functions are combined and regularized to balance the tradeoff between model complexity and data fit.

## Background: Optimization
- To find the best model parametrization, we minimize the loss function with respect to the model parameters. 
- That is we compute, $\mathcal{L^\star} \defeq \inf_{\theta \in \Theta} \mathcal{L}(\theta)$ assuming an infimum exists.
- To converge to a minima the optimizer needs an oracle $\mathcal{O}$, i.e. evaluation of the Loss function, its gradients, or higher order derivatives.
Then, for an algortihm $\mathcal{A}$,
$$
  \theta ^ {t+1} 
  \defeq \mathcal{A}(\theta ^ 0, \ldots, \theta ^ t, 
    \mathcal{O}(\theta ^ 0), \ldots, \mathcal{O}(\theta ^ t), \lambda),
$$
where $\lambda \in \Lambda$ is a hyperparameter.

Stochastic Gradient Descent, Adam, and RMSProp are common optimization algorithms for training neural networks.

## Background: Meta-Learning

:::: {layout="[ 50, 50 ]"}

::: {#first-column}
- Improve the learning algorithm itself given the experience of mutiple learning episodes.
- Base learning: an inner learning algorithm solves a task defined by a dataset and objective. 
- Meta-learning: an outer algorithm updates the inner learning algorithm.
:::

::: {#second-column}
![](figures/BG1/Metalearning.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::
Algorithms for meta-learning are still in a nascent stage with significant computational overhead.

::: {.attribution}
Figure by [Dr. John Lindner ](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Background: Physics-Informed Neural Networks(PiNNs)
- Synthesyzing data with differential equation constraints.
- Physics as a weak constraint in a composite loss function or a hard constraint with architectural choices.
- Symplectic constraints to the loss function gives Hamiltonian Neural Networks.

<img src="figures/BG1/HNN_schema.png" alt="HNN schema" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">

::: {.attribution}
Figure by [Greydanus et al.](https://arxiv.org/abs/2006.08656)
:::

## Background: Coordinates matter

<!-- {{<video figures/coordinates.mp4 width="500" height="400" autoplay="true" loop="true">}} -->
<div>
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG1/coordinates.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>
Neural networks are coordinate dependent. The choice of coordinates can significantly affect the performance of the network.

::: {.attribution}
Adapted from Heliocentrism and Geocentrism by [Malin Christersson
](https://www.malinc.se/math/)
:::


# Metalearning Activation functions 
(Published and Patented)
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script><div class="altmetric-embed" data-badge-type="large-donut" data-altmetric-id="126463806"></div>

## Foundation {.center}

- Most complex systems showcase diversity in populations.
- Artificial neural network architectures are traditionally layerwise-homogenous due to computational efficiencies.
- Will neurons diversify if given the opportunity?



## Insight
:::: {layout="[ 65, 35 ]"}

::: {#first-column}
::: {.center-xy}
- Activation functions can themselves be modeled as neural networks.
- Activation function subnetworks are optimizable via metalearning.
- Multiple subnetwork initializations allow the activations to _diversify_ and evolve into different communities.
:::
:::

::: {#second-column}
![](figures/Result1/meta_act_schema_vert.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Methodology
:::: {layout="[ 60, 40 ]"}

::: {#first-column}

- Developed a metalearning framework to optimize activation functions.
- Tested the algorithm on classification and regression tasks for conventional and physics-informed neural networks.
- Showed a regime where learned diverse activations are superior.
- Gave preliminary analysis to support diversity in activation functions improving performance.

:::

::: {#second-column}

![](figures/Result1/2NeuronSubpopulation_Sine_1_compact2.png){fig-alt="2neuronmnist1d" fig-align="center" width=100%}

:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Results: Scaling
![](figures/Result1/meta_div_scaling.png){fig-alt="scaling" fig-align="center" width=80%}


::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Results Real World Data
![](figures/Result1/real_pendulum.png){fig-alt="real world example" fig-align="center" width=80%}

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Analysis: Participation Ratio
Estimate change in dimensionality of network activations

:::: {layout="[ 65, 35 ]"}

::: {#first-column}

$$
Nr = \mathcal{R} 
    = \frac{(\operatorname{tr}{\bf C})^2}{\operatorname{tr}{\bf C}^2}
    = \frac{\left(\sum_{n=1}^N\lambda_n \right)^2}{\sum_{n=1}^N \lambda_n^2}
$$

where $\lambda_n$ are the co-variance matrix $\bf C$'s eigenvalues for neuronal activity data matrix.
The normalized participation ratio $r = \mathcal{R} / N$.

:::

::: {#second-column}

![](figures/Result1/Participation_ratio.png){fig-alt="participation ratio" fig-align="center" width=100%}

:::
::::

Diverse activation functions use more of the network's capacity.
<!-- The participation ratio is significantly greater for the diverse activation functions indicating a greater use of the network's capacity. -->

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

::: {.notes}
If all the variance is in one dimension, say $\lambda_n = \delta_{n1}$, then $\mathcal{R} = 1$

If the variance is evenly distributed across all dimensions, so $\lambda_n = \lambda_1$, then $\mathcal{R} = N$. 

Typically, $1 < \mathcal{R} < N$, and $\mathcal{R}$ corresponds to the number of dimensions needed to explain most of the variance
:::

## Conclusions

- Learned Diversity Networks discover sets of activation functions that can outperform traditional activations.
- These networks excel in the regime of low data few shot learning.
- Due to the nature of metalearning, the memory overhead is significant concern for scaling.

## Speculations: Achieving stable minima
Optimization of a neural network with shuffled data is akin to noisy descent in the loss landscape.
This can me modeled with the Langevin equation:
$$
\DeclareMathOperator{\Tr}{Tr}
d\theta_{t} = - \nabla \mathcal{L}(\theta_{t})\, dt + \sqrt{2\mathbf{D}} \cdot d\mathcal{W}_{t}
$$
with noise intensity $\mathbf{D} = (\eta / B) \mathcal{L}(\theta) \mathbf{H}(\theta^*)$

Resulting minima from diverse neurons is flatter than from homogeneous ones, as measured by both the trace $\Tr\mathbf{H}$ of the Hessian and the fraction $f$ of its eigenvalues near zero: $\Tr \mathbf{H}_1 >  \Tr \mathbf{H}_2 > \Tr \mathbf{H}_{12}$ and $f_1 < f_2 < f_{12}$

Since the noise term aligns with Hessian bear the minima, this suggests that the minima found by diverse neurons are flatter and more stable.

## Speculations: Structure and universality to diverse activations

- Learned activation functions appear qualitatively independent of the base activation function.
- The odd and even nature of the learned functions suggest that the network is learning to span the space of possible activation functions.

![](figures/Result1/learned_act_insight.png){fig-alt="spanning sets" fig-align="center" width=80%}



# Control and Chaos
<!-- ![](assets/chaos_xkcd.png){fig-alt="Machine Learning XKCD" fig-align="center" height="350px"}

::: {.attribution}
XKCD comic by [Randall Munroe](https://xkcd.com/1399/)
::: -->

## Background: Hamilton Jacobi Bellman(HJB) Equation
Extension of Hamilton Jacobi equation/ Continous time analog of Dynamic Programming.

$$
\dfrac{dx}{dt} = f(x(t),u(t))dt \\
J(x,u,t_0,t_f) = Q(x(t_f), t_f) + \int_{t_0}^{t_f} \mathcal{L}(x(\tau),u(\tau))d\tau\\
V(x(t_0),t_0,t_f) = \min_{u(t))} J(x(t_0),u(t),t_0,t_f)\\
-\dfrac{\partial V}{\partial t} = \min_{u(t)} \left[ \mathcal{L}(x(t),u(t)) + \dfrac{\partial V}{\partial x}^T f(x(t),u(t)) \right]
$$

## Background: Pontryagin's Maximum Principle

## Background: Model Predictive Control

## Background: Chaos

## Background: Chaos Control

## Background: Pendulum
<!-- The career of a young theoretical physicist consists of treating the harmonic oscillator in ever-increasing levels of abstraction. - Sidney Coleman -->

## Background: Kuramoto Oscillator

## Background: Array Systems
<div>
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/Chaotic_pendulum_array.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>

# Neural Network control of chaos
## Foundation

## Insight
- Optimal control of network dynamics involves minimizing a cost functional.
- Traditional control approaches like Pontryagin's maximum principle or Hamilton-Jacobi-Bellman equations are analytically and computationally intractable for complex systems.
- Neural networks based on neural ODES can approximate the optimal control policy.

## Methodology

![](figures/Result2/MPC.png){fig-alt="Model Predictive control" fig-align="center" width=100%}

## Results (Kuramoto Grid)

::: {layout-nrow=2}
![](figures/Result2/Uncontrolled_kuramoto_dynamics.png){fig-alt="Uncontrolled Kuramoto dynamics" fig-align="top" width=100%}

![](figures/Result2/NN_controlled_kuramoto_dynamics.png){fig-alt="NN controlled Kuramoto dynamics" fig-align="top" width=100%}

![](figures/Result2/Uncontrolled_kuramoto_dynamics_noisy_0.1.png){fig-alt="Uncontrolled Kuramoto dynamics with noise" fig-align="bottom" width=100%}

![](figures/Result2/NN_controlled_kuramoto_dynamics_noisy_0.1.png){fig-alt="NN controlled Kuramoto dynamics with noise" fig-align="bottom" width=100%}
:::

## Results: (Pendulum Array)
![](figures/Result2/controlled_pendulum_dynamics.png){fig-alt="Controlled Pendulum dynamics" fig-align="center" width=100%}


## Future Work Exotic Dynamics
<div>
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/Result2/ks.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>
<!-- <div>
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/Result2/ks_chimera.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div> -->
## Speculation

# Dynamics, symmetries, and introspection
## Background: Symmetries

## Background: Dynamical Systems

## Background: Controllers for Dynamical systems

## Background: Group Equivariant Neural Networks

# Symmetries of controllers
## Foundation

## Insight

## Methodology

## Expectation

# Conclusions

## Conclusion

## Deliverables

## Acknowledgements {background-image="assets/ks_pretty.png" background-size="contain" background-position="center" background-repeat="no-repeat}

:::{.attribution}
Background image from [Jacqueline Doan](https://community.wolfram.com/groups/-/m/t/2509110)
:::

# Backup slides {.unnumbered .unlisted visibility="uncounted"}

## Universal approximation theorem {.unnumbered .unlisted visibility="uncounted"}

## Stochastic gradient descent {.unnumbered .unlisted visibility="uncounted"}

## Neural ODEs {.unnumbered .unlisted visibility="uncounted"}

## HJB Derivation Sketch {.unnumbered .unlisted visibility="uncounted"}
Bellman optimality equation:
$$
V(x(t_0),t_0,t_f) = V(x(t_0),t_0,t) + V(x(t),t,t_f)
$$
$$
\begin{align*}
  \dfrac{dV(x(t),t,t_f)}{dt} &= \dfrac{\partial V}{\partial t} + \dfrac{\partial V}{\partial x}^T \frac{dx}{dt}\\
  &= \min_{u(t)} \dfrac{d}{dt} \left[ \int_0^{t_f}\mathcal{L}(x(\tau),u(\tau))d\tau + Q(x(t_f), t_f) \right]\\
  &= \min_{u(t)} \left[ \dfrac{d}{dt}\int_0^{t_f}\mathcal{L}(x(\tau),u(\tau))d\tau \right] \\
\end{align*}
$$

$$
\implies \boxed{-\dfrac{\partial V}{\partial t} = \min_{u(t)} \left[ \mathcal{L}(x(t),u(t)) + \dfrac{\partial V}{\partial x}^T f(x(t),u(t)) \right]}
$$

## Stochasticity and noise {.unnumbered .unlisted visibility="uncounted"}

## Delay coordinates and noise {.unnumbered .unlisted visibility="uncounted"}

## Neural network architectures {.unnumbered .unlisted visibility="uncounted"}