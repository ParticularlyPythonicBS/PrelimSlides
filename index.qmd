---
title: Nonlinear dynamics with neural networks
subtitle: Preliminary Examination
author: Anil Radhakrishnan
institute: Nonlinear Artificial Intelligence Laboratory, North Carolina State University
date: last-modified
bibliography: references.bib
csl: nature.csl

format:
  revealjs:
    theme: [default, assets/custom.scss]
    logo: assets/NAIL_state.png
    css: assets/style.css
    slide-number: true
    toc: true
    toc-depth: 1
    show-slide-number: all
    chalkboard: true
    progress: true
    touch: true
    keyboard: true
    mouse-wheel: true
    controls: auto
    auto-play-media: true
    citations-hover: true
    reference-location: section
    include-in-header: 
      text: |
        <style>
        .center-xy {
          margin: 0;
          position: absolute;
          top: 0%;
          -ms-transform: translateY(20%);
          transform: translateY(20%);
        }
        </style>
revealjs-plugins:
  - attribution
    
---

# Motivation

## Motivation: Why Neural Networks?
- The preeminent computational tool of the contemporary world.
- Differentiable, optimizable, and scalable.
- Emergent uses in and out of Physics @vicentini_netket_2022.

::: {layout-ncol=3}

![](assets/ChatGPT.png){fig-alt="ChatGPT" fig-align="left" width="250"}

![](assets/Microsoft_365_Copilot_Icon.png){fig-alt="CoPilot" fig-align="center" width="250"}

![](assets/netket.png){fig-alt="NetKet" fig-align="right" width="250"}

:::

::: {.notes}

- Neural networks have established themselves as a great tool for a variety of tasks. 
- They are not an answer to everything, but their scalability makes them a great tool in the age of big compute, big data, and big models.
- They have also found their place in fundamental science, both speeding up research and enabling new discoveries.

:::
  
## Motivation: Why Nonlinear Dynamics?
- Captures complexity.
- Formal, well-established framework.
- Emergent uses in and out of Physics.

<div style="margin-top: -70px; margin-bottom: -100px;">
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="assets/nonlinear_dynamics.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>


<p style="font-size: 0.5em;">
Avalanche activity cascades in a sandpile automaton | Vortex street formed by flow past a cylinder | Turing patterns in a reaction-diffusion model
</p>

::: {.attribution}
Animation by [William Gilpin](https://github.com/williamgilpin/cphy)
:::

::: {.notes}

Nonlinear dynamics is a powerful paradigm for understanding complex systems. And more and more systems are becoming more and more complex as we move forward.

Avalanche activity cascades in a sandpile automaton; a vortex street formed by flow past a cylinder; and Turing patterns in a reaction-diffusion model.

:::

## Motivation: Why Nonlinear Dynamics for Neural Networks?

- Optimization is inescapably nonlinear @sohl-dickstein_boundary_2024.
- Neural networks are inherently nonlinear.
- Scope of nonlinearities in neural networks is underexplored.

<table style="width:100%; margin:0; padding:0;">
  <tr>
    <td style="vertical-align:top; padding-right:20px; width:80%">

<video width="700" height="300" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="assets/neural_network_fractals_tanh.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

    </td>
    <td style="vertical-align:top; width:20%">
![](assets/chaos_cm.png){fig-alt="Model Predictive control" fig-align="center" width=50%}
    </td>
  </tr>
</table>


::: {.attribution}
Trainability fractal by [Jascha Sohl-Dickstein](https://sohl-dickstein.github.io/2024/02/12/fractal.html)
:::

::: {.notes}
Each pixel corresponds to training the same neural network from the same initialization on the same data — but with different hyperparameters. Blue-green colors mean that training converged for those hyperparameters, and the network successfully trained. Red-yellow colors mean that training diverged for those hyperparameters. The paler the color the faster the convergence or divergence

The neural network consists of an input layer, a tanh nonlinearity, and an output layer. In the image, the x-coordinate changes the learning rate for the input layer’s parameters, and the y-coordinate changes the learning rate for the output layer’s parameters.
:::

## Motivation: Why Neural Networks for Nonlinear Dynamics?
- Nonlinear dynamical systems are computationally expensive to solve.
- Paradigm of solution as an element of a distribution translates naturally to neural networks.
- Data-driven methods accommodate realistic complexity.

::: {.notes}

:::

# Background Neural Networks and optimization

## Background: Differentiable Computing

- Paradigm where programs can be differentiated end-to-end automatically, enabling optimization of parameters of the program @blondel_elements_2024.
- Techniques to differentiate through complex programs is more than just deep learning.
- Can be interpreted probabilistically or as a dynamical system.
![](figures/BG1/autodiff.png){fig-alt="autodifferentiation diagram" width=80% }

::: {.notes}

- For the purposes of this study, it is better to think of these algorithms through the lens of differentiability.
- The large parameter space afforded by network structures are ripe for gradient based optimization.

:::

## Background: Neural Networks

![](figures/BG1/NN.png){fig-alt="Neural Network diagram" width=80%}
<div style="margin-top: -20px;">
$$
\newcommand{\defeq}{\mathrel{\mathop:}=}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Param}{\theta}
\newcommand{\Loss}{\mathcal{L}}
$$

$$\varphi: \R^d \rightarrow \R^N_L \qquad T^l: \R^{N_{l-1}}\rightarrow \R^{N_{l}} \qquad \sigma: \R\rightarrow \R$$
$$x \in \R^d \qquad W^l \in \R^{N_l\times N_{l-1}} \qquad b^l \in \R^{N_l}$$

::: {.notes}

The neural network is a composition of functions.
We can swap out any of these components and and as long as its composable and differentiable, we can optimize it and call it a neural network.

:::

## Background: Backpropagation

Consider a differentiable loss function $\Loss$,

:::: {layout="[ 40, 60 ]"}

::: {#first-column}
$$
\begin{eqnarray*}
\delta^L =& \nabla_{\varphi}\Loss\odot\sigma'(T^L)\\
\delta^l =& ((w^{l+1})^\intercal \delta^{l+1} ))\odot\sigma'(T^l)\\
\dfrac{\partial \Loss}{\partial b^l_j} =& \delta_j^l \\
\dfrac{\partial \Loss}{\partial w^l_{jk}} =& T^{l-1}_k\delta_j^l \\
\end{eqnarray*}
$$
:::

::: {#second-column}
![](figures/BG1/backprop_zoomed.png){fig-alt="backprop diagram" fig-align="right" width=80%}
:::
::::
The tunable parameters can then be updated by:
$\Param\gets \Param -\eta \nabla_{\Param}\Loss$

::: {.notes}

- Computational efficiency aside, backpropagation is essentially the chain rule.
- We take the derivative of the loss with respect to the output of the network and then backpropagate the gradient through the network.
- In the simplest case, we just subtract the gradient from the parameters to update them.

:::

## Background: Loss Functions
- Neural networks compute a probability distribution on the data space. 
- Our objective is maximize the likelihood the assigned to empirical data.
- Constructing a suitable differentiable loss function gives the path to optimization of a neural network.
Common loss functions include:
  - Mean Squared Error $\Loss(\theta) = \sum_{i=1}^N (y_i - f(x_i;\theta))^2$
  - Cross Entropy $\Loss(\theta) = -\sum_{i=1}^N y_i \log(f(x_i;\theta))$
  - Kullback-Leibler Divergence $\Loss(\theta) = \sum_{i=1}^N y_i \log\left(\dfrac{y_i}{f(x_i;\theta)}\right)$
  
Loss functions are combined and regularized to balance the tradeoff between model complexity and data fit.

::: {.notes}

- The art of crafting the loss function can make or break a model.
- The loss needs to be computationally efficient, and smooth enough to allow for stable gradient based optimization. 

:::

## Background: Optimization
- To find the best model parametrization, we minimize the loss function with respect to the model parameters. 
- That is we compute, $\mathcal{L^\star} \defeq \inf_{\theta \in \Theta} \Loss(\theta)$ assuming an infimum exists.
- To converge to a minima the optimizer needs an oracle $\mathcal{O}$, i.e. evaluation of the Loss function, its gradients, or higher order derivatives.
Then, for an algorithm $\mathcal{A}$,
$$
  \theta ^ {t+1} 
  \defeq \mathcal{A}(\theta ^ 0, \ldots, \theta ^ t, 
    \mathcal{O}(\theta ^ 0), \ldots, \mathcal{O}(\theta ^ t), \lambda),
$$
where $\lambda \in \Lambda$ is a hyperparameter.

Stochastic Gradient Descent, Adam, and RMSProp are common optimization algorithms for training neural networks.

::: {.notes}

- The optimizer is an important choice in the training process.
- Optimizer takes in loss related info and updates the parameters.

Adam stands for adaptive moment estimation and is a stochastic gradient descent optimization algorithm that uses an adaptive learning rate based on estimates of the first and second moments. It maintains exponential moving averages of the weights and gradients, which it uses to scale the learning rate. In other words, Adam uses estimates of the mean and variance of the gradients to adaptively scale the learning rate during training, which can improve the speed and stability of the optimization process.

When you want a fast and efficient optimization algorithm: Adam requires relatively little memory and computation, making it a fast and efficient choice for training deep learning models.
When you have noisy or sparse gradients: Adam is well-suited for optimizing models with noisy or sparse gradients, as it can use the additional information provided by the gradients to adapt the learning rate on the fly.

The algorithm is an extension of the famous stochastic gradient descent (SGD) algorithm. The key idea behind RMSProp is to scale the gradient of each weight in the model by dividing it by the root mean square (RMS) of the gradients of that weight. This helps prevent weights with high gradients from learning too quickly while allowing weights with low gradients to continue learning faster.
If you are training a model with many parameters and are experiencing issues with the model diverging or oscillating during training, RMSProp can help stabilize the training process by adjusting to the gradient.

:::

## Background: Meta-Learning

:::: {layout="[ 50, 50 ]"}

::: {#first-column}
- Improve the learning algorithm itself given the experience of multiple learning episodes.
- Base learning: an inner learning algorithm solves a task defined by a dataset and objective. 
- Meta-learning: an outer algorithm updates the inner learning algorithm.
:::

::: {#second-column}
![](figures/BG1/Metalearning.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::
Algorithms for meta-learning are still in a nascent stage with significant computational overhead.

::: {.notes}

Meta-learning allows the learning algorithm to optimize itself by adding a further layer of abstraction.

:::

::: {.attribution}
Figure by [John Lindner ](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Background: Physics-Informed Neural Networks(PiNNs)
- Synthesizing data with differential equation constraints.
- Physics as a weak constraint in a composite loss function or a hard constraint with architectural choices.
- Symplectic constraints to the loss function gives Hamiltonian Neural Networks @greydanus_hamiltonian_2019.

<img src="figures/BG1/HNN.png" alt="HNN schema" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">

::: {.notes}

One way to integrate physics into the neural network is to add the physics as a constraint to the loss function. This can be done by adding a term to the loss function that penalizes the network for not satisfying the physics. This is known as a weak constraint because the network is not required to satisfy the physics exactly, but rather to minimize the difference between the physics and the network's predictions.

:::

## Background: Coordinates matter

<div>
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG1/coordinates.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>
Neural networks are coordinate dependent. The choice of coordinates can significantly affect the performance of the network.

::: {.attribution}
Adapted from Heliocentrism and Geocentrism by [Malin Christersson
](https://www.malinc.se/math/)
:::

::: {.notes}
As with any data driven modelling, the way you represent the data can have a significant impact on the performance of the model.
:::

# Metalearning Activation functions

(Published, US and International Patent Pending)
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script><div class="altmetric-embed" data-badge-type="large-donut" data-altmetric-id="126463806"></div>

## Foundation

- Most complex systems showcase diversity in populations.
- Artificial neural network architectures are traditionally layerwise-homogenous
- Will neurons diversify if given the opportunity?

## Insight
:::: {layout="[ 65, 35 ]"}

::: {#first-column}
::: {.center-xy}
- Activation functions can themselves be modeled as neural networks.
- Activation function subnetworks are optimizable via metalearning.
- Multiple subnetwork initializations allow the activations to _diversify_ and evolve into different communities.
:::
:::

::: {#second-column}
![](figures/Result1/meta_act_schema_vert.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Methodology

:::: {layout="[ 60, 40 ]"}

::: {#first-column}
- Developed a metalearning framework to optimize activation functions.
- Tested the algorithm on classification and regression tasks for conventional and physics-informed neural networks.
- Showed a regime where learned diverse activations are superior.
- Gave preliminary analysis to support diversity in activation functions improving performance.
:::

::: {#second-column}
![](figures/Result1/2NeuronSubpopulation_Sine_1_compact2.png){fig-alt="2neuronmnist1d" fig-align="center" width=100%}
:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Results: Scaling

![](figures/Result1/meta_div_scaling.png){fig-alt="scaling" fig-align="center" width=80%}


::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

::: {.notes}
Using 1DMNIST dataset, we showed that the learned diverse activations outperformed the traditional activations in the regime of low data few shot learning of around 5 epochs.
These results are averaged over 100 initializations.
:::

## Results: Real World Data

![](figures/Result1/real_pendulum.png){fig-alt="real world example" fig-align="center" width=80%}

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

::: {.notes}

We also showed that it can work for real world data, in this case a real pendulum. 
We used an HNN for this task to further illustrate the generalizability of the learned diverse activations.

:::

## Analysis: Participation Ratio
Estimate change in dimensionality of network activations

:::: {layout="[ 65, 35 ]"}

::: {#first-column}

$$
Nr = \mathcal{R} 
    = \frac{(\operatorname{tr}{\bf C})^2}{\operatorname{tr}{\bf C}^2}
    = \frac{\left(\sum_{n=1}^N\lambda_n \right)^2}{\sum_{n=1}^N \lambda_n^2}
$$

where $\lambda_n$ are the co-variance matrix $\bf C$'s eigenvalues for neuronal activity data matrix.
The normalized participation ratio $r = \mathcal{R} / N$ @gao_theory_2017.

:::

::: {#second-column}

![](figures/Result1/Participation_ratio.png){fig-alt="participation ratio" fig-align="center" width=100%}

:::
::::

Diverse activation functions use more of the network's capacity.
<!-- The participation ratio is significantly greater for the diverse activation functions indicating a greater use of the network's capacity. -->

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

::: {.notes}
If all the variance is in one dimension, say $\lambda_n = \delta_{n1}$, then $\mathcal{R} = 1$

If the variance is evenly distributed across all dimensions, so $\lambda_n = \lambda_1$, then $\mathcal{R} = N$. 

Typically, $1 < \mathcal{R} < N$, and $\mathcal{R}$ corresponds to the number of dimensions needed to explain most of the variance
:::

## Conclusions

- Learned Diversity Networks discover sets of activation functions that can outperform traditional activations.
- These networks excel in the regime of low data few shot learning.
- Due to the nature of metalearning, the memory overhead is significant concern for scaling.

## Speculations: Achieving stable minima

<div style="margin-bottom: 30px;">
![](figures/Result1/Noisy_descent.png){fig-alt="Noisy descent" fig-align="center" width=30%}
</div>

&nbsp;

Optimization of a neural network with shuffled data is a noisy descent.

This can be modeled with the Langevin equation:

$$
\DeclareMathOperator{\Tr}{Tr}
d\theta_{t} = - \nabla \Loss(\theta_{t})\, dt + \sqrt{2\mathbf{D}} \cdot d\mathcal{W}_{t}
$$
with noise intensity $\mathbf{D} = (\eta) \Loss(\theta) \mathbf{H}(\theta\star)$ @mori_power-law_2022

::: {.notes}

Minima from diverse neurons is flatter than from homogeneous ones, as measured by both the trace $\Tr\mathbf{H}$ of the Hessian and the fraction $f$ of its eigenvalues near zero: $\Tr \mathbf{H}_1 >  \Tr \mathbf{H}_2 > \Tr \mathbf{H}_{12}$ and $f_1 < f_2 < f_{12}$

Since the noise term aligns with Hessian bear the minima, this suggests that the minima found by diverse neurons are flatter and more stable.

:::

## Speculations: Structure and universality to diverse activations

- Learned activation functions appear qualitatively independent of the base activation function.
- The odd and even nature of the learned functions suggest that the network is learning to span the space of possible activation functions.

![](figures/Result1/learned_act_insight.png){fig-alt="spanning sets" fig-align="center" width=80%}

::: {.notes}

The learned activations seem to form spanning sets for 1-d activation functions.
There can be future work to explore the universality of this with multidimensional activations and large activation communities. But computational overhead is a significant concern.

:::

# Background: Control and Chaos

## Background: Hamilton Jacobi Bellman(HJB) Equation

$$
\dfrac{dx}{dt} = f(x(t),u(t))dt \\
\mathcal{H}(x,u,t_0,t_f) = Q(x(t_f), t_f) + \int_{t_0}^{t_f} \Loss(x(\tau),u(\tau))d\tau\\
V(x(t_0),t_0,t_f) = \min_{u(t)} \mathcal{H}(x(t_0),u(t),t_0,t_f)\\
-\dfrac{\partial V}{\partial t} = \min_{u(t)} \left[ \Loss(x(t),u(t)) + \dfrac{\partial V}{\partial x}^T f(x(t),u(t)) \right]
$$

:::{.notes}
V(x,t) is the value function, the minimum cost-to-go from state x at time t.
If we can solve for V then we can find from it a control u that achieves the minimum cost.

Extension of Hamilton Jacobi equation/ Continuous time analog of Dynamic Programming.
:::

## Background: Model Predictive Control

Control scheme where a model is used for predicting the future behavior of the system over finite time window @fiedler_-mpc:_2023.

<table style="width:100%; margin:0; padding:0;">
  <tr>
    <td style="vertical-align:top; padding-right:20px; width:65%">
<video width="700" height="300" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/BG2/MPC.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<video width="200" height="200" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/disc_3d_ctrl_motor.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
    <td style="vertical-align:top; width:35%">
![](figures/BG2/MPC_basic.png){fig-alt="Model Predictive control" fig-align="center" width=100%}
    </td>
  </tr>
</table>

::: {.attribution}
Animations from [do-mpc documentation](https://www.do-mpc.com/en/latest/theory_mpc.html)
:::

::: {.notes}
Based on these predictions and the current measured/estimated state of the system, the optimal control inputs with respect to a defined control objective and subject to system constraints is computed. 
After a certain time interval, the measurement, estimation and computation process is repeated with a shifted horizon

Proactive control action: The controller is anticipating future disturbances, set-points etc.

Non-linear control: MPC can explicitly consider non-linear systems without linearization

Arbitrary control objective: Traditional set-point tracking and regulation or economic MPC

constrained formulation: Explicitly consider physical, safety or operational system constraints
The dotted line indicates the current prediction and the solid line represents the realized values.

This simulation uses moving horizon estimation with the problem discretized by orthogonal collocation.
:::

## Background: Chaos

Let $X$ be a metric space. A continuous map $f: X \rightarrow X$ is said to be chaotic on $X$ if:

<table style="width:100%">
  <tr>
    <td style="vertical-align:top">

- $f$ has sensitive dependence on initial conditions
- is topologically transitive
- and has dense periodic orbits @devaney_introduction_2003

    </td>
    <td style="vertical-align:top">
<video width="600" height="700" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/lorentz_attractor.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
  </tr>
</table>

:::{.notes}
Devaney's definition of chaos is a general useful definition for our purposes. Any two of the rules give the other in ℝ.

There is an equivalent definition where f is indecomposable and the periodic points are dense in X.
:::

## Background: Traditional Chaos Control

Relies on the ergodicity of chaotic systems [@ott_controlling_1990; @ditto_experimental_1990].

![](figures/BG2/OGY.png){fig-alt="Ott-Gerbogi-Yorke control" fig-align="center" width=80%}

![](figures/BG2/UPO.png){fig-alt="Unstable periodic orbits" fig-align="center" width=40%}

:::{.notes}

OGY relies on stabilizing unstable periodic orbits in the chaotic system.
When a trajectory is near the UPO, the OGY method uses a perturbation to move the trajectory off the unstable manifold of the UPO and onto the stable manifold.

In the neighborhood of the UPO, the next point of the system’s Poincar´e section can be
approximated using the Jacobian. And then the shift can be approximated by linear approximation

Shown attractor of the duffing oscillator with the UPOs marked in purple.
:::

## Background: Chaotic Pendulum Array @braiman_taming_1995
<div>
<video width="700" height="400" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/Chaotic_pendulum_array.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>

$$
l_n^2\ddot\theta_n =-\gamma\dot\theta_n\nonumber-l_n\sin{\theta_n}\nonumber +\tau_{0}+\tau_{1}\sin{\omega t}\nonumber +\kappa(\theta_{n-1}+\theta_{n+1}-2\theta_n)
$$

:::{.notes}

This damped driven pendulum array is a classic example of a chaotic system that was deeply explored by Y. Braiman, John and Bill in the 90s exploring disorder taming chaos.

If we remove the coupling we get a simple driven damped pendulum. This system can itself be chaotic as well.

:::

## Background: Kuramoto Oscillator @kuramoto_international_1975

<div>
<table style="width:100%; margin:0; padding:0;">
  <tr>
    <td style="vertical-align:top; padding-right:20px; width:65%">
<video width="300" height="600" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/BG2/fireflies_sim.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
    <td style="vertical-align:top; width:35%">
<video width="700" height="300" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/BG2/firefly_vid.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
$$\dot{\theta}_i = \omega_i + \lambda\sum_{j=1}^N \sin(\theta_j - \theta_i)$$
    </td>
  </tr>
</table>
</div>

:::{.attribution}
Synchronizing fireflies video from [Robin Meier](http://robinmeier.net/?p=2279)
:::

:::{.notes}
Kuramoto can be thought of as a first order mean field theory of the complex Ginzburg-Landau equation.

This model was initially proposed to capture synchronization in large weakly-coupled oscillator systems.
It found a lot of applications like josephson junction arrays, power grids, and even modelling fireflies.

:::

# Neural Network control of chaos

## Insight
- Optimal control of network dynamics involves minimizing a cost functional.
- Traditional control approaches like Pontryagin's (maximum) principle or Hamilton-Jacobi-Bellman equations are analytically and computationally intractable for complex systems.
- Neural networks based on neural ODES can approximate the optimal control policy @bottcher_ai_2022.

## Methodology

![](figures/Result2/MPC_NN.png){fig-alt="Model Predictive control" fig-align="center" width=100%}

:::{.notes}
Instead of training a neural network with a predefined dataset, we train the network in situ with the network serving as the controller that is integrated in the dynamical system at each epoch.

The time horizon for the integration is changed from the first half and second half of training by 5x to allow extrapolation of the controller.

:::

## Results: Kuramoto Grid

![](figures/Result2/kuramoto_control.png){fig-alt="Controlled kuramoto dynamics" fig-align="center" width=100%}

:::{.notes}

The simplest test for this method was a grid of kuramoto oscillators. The network was trained to control the phase of the oscillators to a desired state.
This works very well.
Interestingly, this method also extends naturally to noisy kuramoto, which is not guaranteed by HJB.

:::

## Results: Pendulum Array

![](figures/Result2/controlled_pendulum_dynamics.png){fig-alt="Controlled Pendulum dynamics" fig-align="center" width=70%}

<div style="margin-top: 30px;">
<video width="700" height="400" style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/Result2/Pendulum_optimal.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>

:::{.notes}

We also tested the method on the chaotic pendulum array into synchrony while maintaining the mean of the lengths.
The network was able to control this system as well navigating out of the sliver of chaos.

:::

## Future Work: Exotic Dynamics @abrams_chimera_2004
::: {style="font-size: 70%;"}
$$\dot{\theta}_i = \omega_i + \lambda\sum_{j=1}^N A_{ij}\sin(\theta_j - \theta_i-\alpha)$$
:::
<div>
<table style="width:100%; margin:0; padding:0; margin-top:-65px;">
  <tr>
    <td style="vertical-align:top; padding-right:20px; width:50%">
<video width="300" height="600" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/Result2/ks_basic.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
    <td style="vertical-align:top; padding-right:20px; width:50%">
<video width="300" height="600" loop style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
  <source src="figures/Result2/ks_chimera.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
    </td>
  </tr>
</table>
</div>

:::{.notes}

This is an extension of kuramoto system where the coupling is nonlocal and the phase difference is shifted by a phase $\alpha$.

It cannot be ascribed to a supercritical instability of the spatially uniform oscillation, because it occurs even if the uniform state is stable. Furthermore, it has nothing to do with the partially locked/partially incoherent states seen in populations of non-identical oscillators with distributed frequencies. There, the splitting of the population stems from the inhomogeneity of the oscillators themselves; the desynchronized oscillators are the intrinsically fastest or slowest ones. Here, all the oscillators are the same.

It would be a good test of the model to see if it can move a system into and out of chimeric states.

:::

## Future Work: Recurrence matrices in $\mathcal{L}$ @kennel_determining_1992

![](figures/Result2/recurrence_matrix_lorenz.png){fig-alt="Recurrence matrix for lorenz attractor" fig-align="center" width=70%}

:::{.notes}

Recurrence matrices are are way to capture the dynamics of a system by looking at the recurrences of states.
This is similar to the notion of delay coordinate embeddings used for approximating attractors.
As you can see here the recurrence matrices are invariant to the delays.

This can be used in the loss function to prescribe the exact dynamics to optimize towards,

:::

# Background: Dynamics and symmetries

## Background: Group theory for Dynamical Systems

Let $\Gamma$ act on $\mathbb{R}^n$ and $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ . Then $f$ is said to be $\Gamma$-equivariant if $f(\gamma x) = \gamma f(x)$ for all $x \in \mathbb{R}^n$ and $\gamma \in \Gamma$ @zee_group_2016.

For dynamical systems, for a fixed point $f(x\star)=0$, $\gamma x\star$ is also a fixed point.

Isotropy subgroup:
Let $v\in \mathbb{R}^n: \Sigma_v = \{\gamma \in \Gamma: \gamma v = v\}$

Fixed pt subspace:
Let $\Sigma \subseteq \Gamma$. $\text{Fix}(\Sigma)=\{v \in \mathbb{R}^n:\sigma v=v\}$

*Thm*: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a $\Gamma$-equivariant map and  $\Sigma \subseteq \Gamma$. Then $f(\text{Fix}(\Sigma))\subseteq\text{Fix}(\Sigma)$.

:::{.notes}
Cyclic group
Dihedral group = Cyclic group + Flips
Symmetric group = m!
S^1 circle group
O(n) Measure preserving transformations

For ODEs that means if $f(x)$ is a solution, then so is $f(\gamma x)$ for all $gamma \in \Gamma$.
So we can classify solutions upto their symmetry.

If a system starts in a symmetric state, it will remain symmetric.

Isotropy subgroup: The set of all group elements which leave the solution unchanged. i.e Fixed-point subspaces are invariant under the flow generated by f

:::

## Background: Group Equivariant Neural Networks

<div style="margin-bottom: 40px; text-align: center;">
<img src="figures/BG3/lift_project.png" alt="Group equivariance works by lifting and projection" style="width: 50%;">
</div>

Group equivariant NN work through lifting convolutions to the _group_ space, and then projecting back to the original space @cohen_group_2016.

:::{.attribution}
Figure adapted from [Bart Smets et al.](https://arxiv.org/abs/2001.09046)
:::

:::{.notes}
One was thats groups ar integrated into neural networks are as an extension of the traditional convolution neural network setup. 
Instead of just lifting to some real space which preserves translations as in traditional CNNs, we lift to a group space which preserves the group symmetries.
:::

# Symmetries of controllers

## Insight

- Dynamical systems often obey symmetries.
- Controllers are essentially dynamical systems.
- Is there a computationally viable mapping between the symmetries of the system and the controller?

## Methodology

- Analyze the controllers from the previous sections for symmetries using group equivariant autoencoders
- Construct controllers that respect the symmetries of the system
- Compare the performance of symmetric to conventional controllers.

## Expectation

- Viability test for group equivariant neural networks in control systems.
- Mapping between the symmetries of the system and the controller.
- Performance analysis of symmetric controllers.

# Conclusion

- Non-traditional treatments of neural networks let us better capture nonlinearity.
- Standard paradigms of Geometry, Statistics, and Algebra for understanding nonlinear systems are augmented by neural networks.
- The interplay of Physics, Mathematics, and Computer Science gives us the best shot at understanding complex systems

## Deliverables
- Publishable article on Neural Network Control of Chaos.
- Thorough study of the symmetries of controllers.
- Codebase for the above studies.
- Dissertation detailing the discoveries and pitfalls found throughout.

Estimated time of completion: August 2025

## Acknowledgements {background-image="assets/ks_pretty.png" background-size="contain" background-position="center" background-repeat="no-repeat}

<div style="text-align: center; margin-top: 100px;">
<img src="assets/acknowledgements.png" alt="Bill, John, Anshul" style="width: 400px;">
</div>

:::{.attribution}
Background image from [Jacqueline Doan](https://community.wolfram.com/groups/-/m/t/2509110)
:::

## References

---
nocite: |
  @*
---

::: {#refs}
:::



# Backup slides {.unnumbered .unlisted visibility="uncounted"}

## Hyperparameters, integrators, and tolerances {.unnumbered .unlisted visibility="uncounted"}

Metalearning Activation functions:

MNIST1D: 1 hidden layer of 100 neurons, activation of 50 hidden neurons. 100 initializations averaged. RMSprop, Real Pendulum: same other than 50 initializations. Pytorch and Jax frameworks used.

Neural Network control of chaos:

Control and dynamics integration via diffrax Jax library for neural ODEs. *TSit5* algorithm for ODE integration with PID controller with rtol 1e-7 and atol 1e-9. *Stratanovich Milstein* solver for SDE integration with PID controller with rtol 1e-7 and atol 1e-9. 1000 epochs Controller training with only 1/5 data for the first half. Implemented in Jax.

:::{.notes}
RMSProp addresses the issue of a global learning rate by maintaining a moving average of the squares of gradients for each weight and dividing the learning rate by this average. This ensures that the learning rate is adapted for each weight in the model, allowing for more nuanced updates.
The general idea is to dampen the oscillations in directions with steep gradients while allowing for faster movement in flat regions of the loss landscape.

Tsit5 - Tsitouras 5/4 Runge-Kutta method. (free 4th order interpolant). Basically Dormand Prince with better coefficients.

Stratanovich Milstein - order 1 strong Taylor scheme
:::


## Universal approximation theorem {.unnumbered .unlisted visibility="uncounted"}

$$M(\sigma) = \text{span}\{\sigma(wx-\theta):w\in \mathbb{R}^n, \theta\in\mathbb{R}\}$$
<!-- For which $\sigma$ is $\max_{x\in K}|f(x)-g(x)|<\epsilon$ true for any $f\in C(\mathbb{R}^n)$, any compact subset $K$ of $\mathbb{R}^n$, and $\epsilon >0$, and some $g \in M(\sigma)$? -->

*Theorem 1*: Let $\sigma \in C(\mathbb{R})$. Then $M(\sigma)$ is dense in $C(\mathbb{R}^n)$, in the topology of uniform convergence on compacta, if and only if $\sigma$ is not a polynomial @leshno_multilayer_1993.

For Neural Networks,

*Theorem 2*: Let $\sigma$ be any continuous sigmoidal function. Then finite sums of the form $G(x) = \sum_{i=1}^N \alpha_i \sigma(w_i^T x + b_i)$ are dense in $C(I_n)$.
i.e. $\forall f\in C(I_n)$ and $\epsilon>0$, there is a sum $G(x)$ such that $\max_{x\in I_n}|f(x)-G(x)|<\epsilon$ @cybenko_approximation_1989.

Informally, at least one neural network exists that can approximate any continuous function on $I_n=[0,1]^n$ with arbitrary precision.

## SGD via gradient flows {.unnumbered .unlisted visibility="uncounted" .smaller}

::: {style="font-size: 78%;"}

Gradient descent: $x_{n+1} = x_n - \gamma\nabla f(x_n)$ where $\gamma > 0$ is the step-size. Gradient flow is the limit where $\gamma\rightarrow 0$.

There are two ways to treat SGD in this regime,

Consider fixed times $t=n\gamma$ and $s=m\gamma$.

:::: {layout="[ 45, 55 ]"}

::: {#first-column}

- Convergence to gradient flow
  
Given the recursion $x_{n+1} = x_n-\gamma\nabla f(x_n) - \gamma\epsilon_n$,

applying this $m$ times, we get:

$$
\begin{align*}
X(t+s) - X(t) &= x_{n+m}-x_n\\
              &= -\gamma \sum_{k=0}^{m-1}\nabla f(X(t+\frac{sk}{m})) - \gamma\sum_{k=0}^{m-1}ε_{k+n}
\end{align*}
$$
in the limit,
$$
\begin{align*}
X(t+s) - X(t) &= -\int_t^{t+s}\nabla f(X(u))du + 0
\end{align*}
$$
which is just the gradient flow equation.

:::

::: {#second-column}

- Convergence to Langevin diffusion

Given the recursion $x_{n+1} = x_n-\gamma\nabla f(x_n) - \sqrt{2\gamma}\epsilon_n$,

applying this $m$ times, we get:

$$
\begin{align*}
X(t+s) - X(t) &= x_{n+m}-x_n\\
              &= -\gamma \sum_{k=0}^{m-1}\nabla f(X(t+\frac{sk}{m}))- \sqrt{2\gamma}\sum_{k=0}^{m-1}ε_{k+n}
\end{align*}
$$
The second term has finite variance $\propto 2s$. When $m$ tends to infinity,

$$
X(t+s) - X(t) =\\
 -\int_t^{t+s} \nabla f(X(u))du + \sqrt{2}[B(t+s)-B(t)]
$$

This limiting distribution is the Gibbs distribution with density $\exp{(-f(x))}$

:::
::: 

:::

:::{.notes}
γ∑m−1k=0εk+n
 has zero expectation and variance equal to γ2m=γs
 times the variance of each εk+n and thus tends to 0
:::

:::{.attribution}
Argument from [Francis Bach](https://francisbach.com/gradient-flows/)
:::

## 1DMNIST {.unnumbered .unlisted visibility="uncounted"}

![](figures/Backup/MNIST1D.png){fig-alt="MNIST1D" fig-align="center" width=70%}

## Neural ODEs {.unnumbered .unlisted visibility="uncounted"}

- Forward propagation:
$$
x(t_1) = \text{ODESolve}(f(x(t),t,\theta), x(t_0), t_0, t_1)\\
\text{Compute loss: } \Loss(x(t_1))\\
a(t_1) = \frac{\partial \Loss}{\partial x(t_1)}\\
$$

- Back propagation:
$$
\begin{bmatrix}
x(t_0) \\
\frac{\partial \Loss}{\partial x(t_0)}\\
\frac{\partial \Loss}{\partial \theta}
\end{bmatrix}
=
\text{ODESolve}\left(\begin{bmatrix}
f(x(t),t,\theta) \\
-a(t)^T \frac{\partial f(x(t),t,\theta)}{\partial x} \\
-a(t)^T \frac{\partial f(x(t),t,\theta)}{\partial \theta} 
\end{bmatrix}
,
\begin{bmatrix}
x(t_1) \\
\frac{\partial \Loss}{\partial x(t_1)}\\
0_{|\theta|}
\end{bmatrix}
,
t_1, t_0
\right)
$$


## HJB Derivation Sketch {.unnumbered .unlisted visibility="uncounted"}
Bellman optimality equation:
$$
V(x(t_0),t_0,t_f) = V(x(t_0),t_0,t) + V(x(t),t,t_f)
$$
$$
\begin{align*}
  \dfrac{dV(x(t),t,t_f)}{dt} &= \dfrac{\partial V}{\partial t} + \dfrac{\partial V}{\partial x}^T \frac{dx}{dt}\\
  &= \min_{u(t)} \dfrac{d}{dt} \left[ \int_0^{t_f}\Loss(x(\tau),u(\tau))d\tau + Q(x(t_f), t_f) \right]\\
  &= \min_{u(t)} \left[ \dfrac{d}{dt}\int_0^{t_f}\Loss(x(\tau),u(\tau))d\tau \right] \\
\end{align*}
$$

$$
\implies \boxed{-\dfrac{\partial V}{\partial t} = \min_{u(t)} \left[ \Loss(x(t),u(t)) + \dfrac{\partial V}{\partial x}^T f(x(t),u(t)) \right]}
$$

## Pontryagin's Principle {.unnumbered .unlisted visibility="uncounted"}
Let $(x\star,u\star)$ be an optimal trajectory-control pair. Then there exists a $\lambda\star$ such that,
$$
\lambda\star =-\dfrac{\partial \mathcal{H}}{\partial x}
$$
and
$$
\mathcal{H}(x\star,u\star,\lambda\star, t) = \min_{u} \mathcal{H}(x\star,u,\lambda\star, t)
$$
and by definition,
$$
x\star = \dfrac{\partial \mathcal{H}}{\partial \lambda}
$$

## Stochasticity and noise {.unnumbered .unlisted visibility="uncounted"}

::: {style="font-size: 80%;"}
We can add intrinsic noise to it by adding a noise term $\xi$ with a strength $\sigma$:
$$ 
\frac{dx}{dt}=f(x,t)+ σ(x,t)ξ
$$

Then the rectangular reimann  construction is equivalent to: $f(t)≈\sum_{i=1}^n f(\hat{x_i})χ_{Δx_i}$

where $f(\hat{x_i})$ are constants.
This approximation is exact in the limit of $n\to\infty$.


For the stochastic case, the rectangular construction is equivalent to: $\phi(t)≈\sum_{i=1}^n \hat{e}_iχ_{Δx_i}$
Then the integral is:
$$
\int_0^T \phi(t) dW_t=\sum_{i=1}^n \hat{e}_i ΔW_i
$$

Here, the coefficients $\hat{e}_i$ are *not* constants but random variables since we are allowed to integrate $σ$ which depends on $X_t$.

:::

## Recurrence plots caveat emptor {.unnumbered .unlisted visibility="uncounted"}

::: {layout-ncol=2}

![Roessler lyap](figures/Backup/roessler_det.svg){fig-alt="Roessler lyap" fig-align="center" width=100%}

![Roessler rec](figures/Backup/roessler_rec.svg){fig-alt="Roessler recurrence" fig-align="center" width=100%}

:::

:::{.attribution}
Lyapunov exponent and Recurrence plot from [Recurrence Plot Pitfalls](http://www.recurrence-plot.tk/pitfalls.php)
:::

## Neural network architectures {.unnumbered .unlisted visibility="uncounted"}

::: {layout-ncol=2}

![Autoencoder](assets/Autoencoder_schema.png){fig-alt="Autoencoder" fig-align="right" width=60%}


![GRU](assets/Gated_Recurrent_Unit_schema.png){fig-alt="GRU" fig-align="left" width=100%}

:::

![CNN](assets/Typical_cnn.png){fig-alt="CNN" fig-align="center" width=100%}

::: {.attribution}

All images from associated wikipedia pages (Autoencoder, GRU, CNN)
:::