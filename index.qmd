---
title: Nonlinear dynamics with neural networks
subtitle: Preliminary Examination
author: Anil Radhakrishnan
institute: North Carolina State University
date: last-modified
format:
  revealjs:
    theme: [default, assets/custom.scss]
    logo: assets/ncstate_red_letter_logo.png
    css: assets/style.css
    slide-number: true
    show-slide-number: all
    chalkboard: true
    progress: true
    touch: true
    keyboard: true
    mouse-wheel: true
    controls: auto
    auto-play-media: true
    citations-hover: true
    reference-location: section
revealjs-plugins:
  - attribution
    
---
# Motivation
## Motivation: Why Neural Networks?
- The preeminent computational tool of the contemporary world.
- Differentable, optimizable, and scalable.
- Emergent uses in and out of Physics.

::: {layout-ncol=3}

![](assets/ChatGPT.png){fig-alt="ChatGPT" fig-align="left" width="250"}

![](assets/Microsoft_365_Copilot_Icon.png){fig-alt="CoPilot" fig-align="center" width="250"}

![](assets/netket.png){fig-alt="NetKet" fig-align="right" width="250"}

:::

::: {.notes}
Speaker notes go here.
:::
  
## Motivation: Why Nonlinear Dynamics?
- Captures complexity.
- Formal, well established framework.
- Emergent uses in and out of Physics.

<div style="padding:50% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/739921904?h=f803d0bbff&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:5%;width:90%;height:90%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Animation by [Wliiam Gilpin](https://github.com/williamgilpin/cphy)
:::

::: {.notes}
Avalanche activity cascades in a sandpile automaton; a vortex street formed by flow past a cylinder; and Turing patterns in a reaction-diffusion model.
:::

## Motivation: Why Nonlinear Dynamics for Neural Networks?
- Optimization is inescapably nonlinear.
- Neural networks are inherently nonlinear.
- Scope of nonlinearities in neural networks is underexplored.
  
<div style="padding:90% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/903855670?h=ca2b077023&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:30%;width:40%;height:40%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Trainability fractal by [Jascha Sohl-Dickstein](https://sohl-dickstein.github.io/2024/02/12/fractal.html)
:::
::: {.notes}
Each pixel corresponds to training the same neural network from the same initialization on the same data — but with different hyperparameters. Blue-green colors mean that training converged for those hyperparameters, and the network successfully trained. Red-yellow colors mean that training diverged for those hyperparameters. The paler the color the faster the convergence or divergence

The neural network consists of an input layer, a tanh nonlinearity, and an output layer. In the image, the x-coordinate changes the learning rate for the input layer’s parameters, and the y-coordinate changes the learning rate for the output layer’s parameters.
:::

## Motivation: Why Neural Networks for Nonlinear Dynamics?
- Nonlinear dynamical systems are computationally expensive to solve.
- Paradigm of solution as an element of a distribution translates naturally to neural networks.
- Data-driven methods accommodate realistic complexity.

<img src="assets/escher_day_night.png" alt="Escher's day and night wood blood painting" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">

  
::: {.attribution}
Day and Night by [M.C. Escher](https://www.wikiart.org/en/m-c-escher/day-and-night)
:::

# Background
## Background: Differentiable Computing
- Programming paradigm which composes parameterized algorithmic components and optimizes them using gradient search.
- Concept emergent from but not limited to deep learning.
- 

## Background: Neural Networks

![](figures/NN.png){fig-alt="Neural Network diagram" width=80%}
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\Param}{\mathcal{P}}
$$

$$\varphi: \R^d \rightarrow \R^N_L \qquad T^l: \R^{N_{l-1}}\rightarrow \R^{N_{l}} \qquad \rho: \R\rightarrow \R$$
$$x \in \R^d \qquad W^l \in \R^{N_l\times N_{l-1}} \qquad b^l \in \R^{N_l}$$

## Background: Backpropagation

Consider a differentiable cost function $C$,

:::: {layout="[ 40, 60 ]"}

::: {#first-column}
$$
\begin{eqnarray*}
\delta^L =& \nabla_{\varphi}C\odot\rho'(T^L)\\
\delta^l =& ((w^{l+1})^\intercal \delta^{l+1} ))\odot\rho'(T^l)\\
\dfrac{\partial C}{\partial b^l_j} =& \delta_j^l \\
\dfrac{\partial C}{\partial w^l_{jk}} =& T^{l-1}_k\delta_j^l \\
\end{eqnarray*}
$$
:::

::: {#second-column}
![](figures/backprop_zoomed.png){fig-alt="backprop diagram" fig-align="right" width=80%}
:::
::::
The tunable parameters can then be updated by:
$$\Param\gets \Param -\eta \nabla_{\Param}C$$

## Background: Loss Functions

## Background: Optimization

## Background: Meta-Learning

## Background: Physics-Informed Neural Networks

# Metalearning Activation functions
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script><div class="altmetric-embed" data-badge-type="large-donut" data-altmetric-id="126463806"></div>

## Foundation

## Insight

## Methodology

## Results (Published and Patented)

## Conclusions

## Speculations

# Background Deux

## Background: Pontryagin's Maximum Principle

## Background: Hamilton-Jacobi-Bellman Equation

## Background: Model Predictive Control

## Background: Chaos

## Background: Chaos Control

## Background: Pendulum
The career of a young theoretical physicist consists of treating the harmonic oscillator in ever-increasing levels of abstraction. - Sidney Coleman

## Background: Kuramoto Oscillator

## Background: Array Systems

## Background: Stochasticity and noise

# Neural Network control of chaos
## Foundation

## Insight

## Methodology

## Results (Preliminary)

## Future Work

## Speculation

# Background (For the last time I promise)
## Background: Symmetries

## Background: Dynamical Systems

## Background: Controllers for Dynamical systems

## Background: Group Equivariant Neural Networks

# Symmetries of controllers
## Foundation

## Insight

## Methodology

## Expectation

# Conclusions

## Conclusion

## Deliverables

<!-- 


## Code Highlighting

For continuous highlighting, use `from-to` (`6-8`).

For discontinuous highlighting, use `line1, line2, ...` (`1, 4`).

To highlight lines in a progressive manner, use `range1|range2` (`|6-8|1,4|`). 

```{.python code-line-numbers="|6-8|1,4|"}
import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

<https://quarto.org/docs/presentations/revealjs/#code-blocks>

## Enable more revealjs features

The theme is built ontop of Quarto's `revealjs` implementation. So, any [features](https://quarto.org/docs/presentations/revealjs/) of available are also usable within the theme. For example, if we wanted to incorporate the [chalkboard](https://quarto.org/docs/presentations/revealjs/presenting.html#chalkboard) feature. We would use:

```yaml
format:
  ncsu-revealjs: 
    chalkboard: true
```

## Summary {#sec-summary}

### NCSU-themed presentation slide deck

The Quarto NCSU Revealjs theme is an extension of Reveal.js and
offers all of its [features](https://quarto.org/docs/presentations/revealjs/) in the context of being brand friendly to North Carolina State University.

Install the theme without this template:

```bash
quarto install extension ParticularlyPythonicBS/ncsu-revealjs
```

Install the theme with the template being present:

```bash
quarto use template ParticularlyPythonicBS/ncsu-revealjs
```

You can learn more about using RevealJS with Quarto at: <https://quarto.org/docs/presentations/revealjs/> -->