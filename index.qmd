---
title: Nonlinear dynamics with neural networks
subtitle: Preliminary Examination
author: Anil Radhakrishnan
institute: North Carolina State University
date: last-modified
format:
  revealjs:
    theme: [default, assets/custom.scss]
    logo: assets/ncstate_red_letter_logo.png
    css: assets/style.css
    slide-number: true
    show-slide-number: all
    chalkboard: true
    progress: true
    touch: true
    keyboard: true
    mouse-wheel: true
    controls: auto
    auto-play-media: true
    citations-hover: true
    reference-location: section
    include-in-header: 
      text: |
        <style>
        .center-xy {
          margin: 0;
          position: absolute;
          top: 0%;
          -ms-transform: translateY(20%);
          transform: translateY(20%);
        }
        </style>
revealjs-plugins:
  - attribution
    
---
# Motivation
## Motivation: Why Neural Networks?
- The preeminent computational tool of the contemporary world.
- Differentable, optimizable, and scalable.
- Emergent uses in and out of Physics.

::: {layout-ncol=3}

![](assets/ChatGPT.png){fig-alt="ChatGPT" fig-align="left" width="250"}

![](assets/Microsoft_365_Copilot_Icon.png){fig-alt="CoPilot" fig-align="center" width="250"}

![](assets/netket.png){fig-alt="NetKet" fig-align="right" width="250"}

:::

::: {.notes}
Speaker notes go here.
:::
  
## Motivation: Why Nonlinear Dynamics?
- Captures complexity.
- Formal, well established framework.
- Emergent uses in and out of Physics.

<div style="padding:50% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/739921904?h=f803d0bbff&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:5%;width:90%;height:90%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Animation by [Wliiam Gilpin](https://github.com/williamgilpin/cphy)
:::

::: {.notes}
Avalanche activity cascades in a sandpile automaton; a vortex street formed by flow past a cylinder; and Turing patterns in a reaction-diffusion model.
:::

## Motivation: Why Nonlinear Dynamics for Neural Networks?
- Optimization is inescapably nonlinear.
- Neural networks are inherently nonlinear.
- Scope of nonlinearities in neural networks is underexplored.
  
<div style="padding:90% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/903855670?h=ca2b077023&autoplay=1&loop=1&title=0&byline=0&portrait=0" style="position:absolute;top:0;left:30%;width:40%;height:40%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

::: {.attribution}
Trainability fractal by [Jascha Sohl-Dickstein](https://sohl-dickstein.github.io/2024/02/12/fractal.html)
:::
::: {.notes}
Each pixel corresponds to training the same neural network from the same initialization on the same data — but with different hyperparameters. Blue-green colors mean that training converged for those hyperparameters, and the network successfully trained. Red-yellow colors mean that training diverged for those hyperparameters. The paler the color the faster the convergence or divergence

The neural network consists of an input layer, a tanh nonlinearity, and an output layer. In the image, the x-coordinate changes the learning rate for the input layer’s parameters, and the y-coordinate changes the learning rate for the output layer’s parameters.
:::

## Motivation: Why Neural Networks for Nonlinear Dynamics?
- Nonlinear dynamical systems are computationally expensive to solve.
- Paradigm of solution as an element of a distribution translates naturally to neural networks.
- Data-driven methods accommodate realistic complexity.

<img src="assets/escher_day_night.png" alt="Escher's day and night wood blood painting" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">

  
::: {.attribution}
Day and Night by [M.C. Escher](https://www.wikiart.org/en/m-c-escher/day-and-night)
:::

# Background
## Background: Differentiable Computing
- Paradigm where programs can be differentiated end-to-end automatically, enabling optimizatio of parameters of the program.
- Techniques to differentiate through complex programs is more than just deep learning.
- Can be interpreted probabilistically or as a dynamical system.
![](figures/autodiff.png){fig-alt="autodifferentiation diagram" width=80% }


## Background: Neural Networks

![](figures/NN.png){fig-alt="Neural Network diagram" width=80%}
$$
\newcommand{\defeq}{\mathrel{\mathop:}=}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Param}{\mathcal{P}}
$$

$$\varphi: \R^d \rightarrow \R^N_L \qquad T^l: \R^{N_{l-1}}\rightarrow \R^{N_{l}} \qquad \rho: \R\rightarrow \R$$
$$x \in \R^d \qquad W^l \in \R^{N_l\times N_{l-1}} \qquad b^l \in \R^{N_l}$$

## Background: Backpropagation

Consider a differentiable cost function $C$,

:::: {layout="[ 40, 60 ]"}

::: {#first-column}
$$
\begin{eqnarray*}
\delta^L =& \nabla_{\varphi}C\odot\rho'(T^L)\\
\delta^l =& ((w^{l+1})^\intercal \delta^{l+1} ))\odot\rho'(T^l)\\
\dfrac{\partial C}{\partial b^l_j} =& \delta_j^l \\
\dfrac{\partial C}{\partial w^l_{jk}} =& T^{l-1}_k\delta_j^l \\
\end{eqnarray*}
$$
:::

::: {#second-column}
![](figures/backprop_zoomed.png){fig-alt="backprop diagram" fig-align="right" width=80%}
:::
::::
The tunable parameters can then be updated by:
$\Param\gets \Param -\eta \nabla_{\Param}C$

## Background: Loss Functions
- Neural networks compute a probability distribution on the data space. 
- Our objective is maximize the likihood the assigned to emperical data.
-Constructing a suitable differentiable loss function gives the path to optimization of a neural network.
Common loss functions include:
  - Mean Squared Error $\mathcal{L}(\theta) = \sum_{i=1}^N (y_i - f(x_i;\theta))^2$
  - Cross Entropy $\mathcal{L}(\theta) = -\sum_{i=1}^N y_i \log(f(x_i;\theta))$
  - Kullback-Leibler Divergence $\mathcal{L}(\theta) = \sum_{i=1}^N y_i \log\left(\dfrac{y_i}{f(x_i;\theta)}\right)$
  
Loss functions are combined and regularized to balance the tradeoff between model complexity and data fit.

## Background: Optimization
- To find the best model parametrization, we minimize the loss function with respect to the model parameters. 
- That is we compute, $\mathcal{L^\star} \defeq \inf_{\theta \in \Theta} \mathcal{L}(\theta)$ assuming an infimum exists.
- To converge to a minima the optimizer needs an oracle $\mathcal{O}$, i.e. evaluation of the Loss function, its gradients, or higher order derivatives.
Then, for an algortihm $\mathcal{A}$,
$$
  \theta ^ {t+1} 
  \defeq \mathcal{A}(\theta ^ 0, \ldots, \theta ^ t, 
    \mathcal{O}(\theta ^ 0), \ldots, \mathcal{O}(\theta ^ t), \lambda),
$$
where $\lambda \in \Lambda$ is a hyperparameter.

Stochastic Gradient Descent, Adam, and RMSProp are common optimization algorithms for training neural networks.

## Background: Meta-Learning

:::: {layout="[ 70, 30 ]"}

::: {#first-column}
- Meta-learning or learning to learn tries to improve the learning algorithm itself given the experience of mutiple learning episodes.
- During base learning , an inner learning algorithm solves a task defined by a dataset and objective. 
- During meta-learning , an outer algorithm updates the inner learning algorithm such that the model it learns improves an outer objective.
:::

::: {#second-column}
![](figures/Metalearning.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::
Algorithms for meta-learning are still in a nascent stage with significant computational overhead.

::: {.attribution}
Figure by [Dr. John Lindner ](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Background: Physics-Informed Neural Networks(PiNNs)
- PiNNs allow for synthesyzing data with differential equation constraints.
- We can incorporate the physics as a weak constraint in a composite loss function or morph the architecture to enforce hard constraints.
- Adding symplectic constraints to the loss function gives Hamiltonian Neural Networks which excel for conservative system predictions.

<img src="figures/HNN_schema.png" alt="HNN schema" style="display: block; width: 30%; margin-top: -25px; margin-left: auto; margin-right: auto;">

::: {.attribution}
Figure by [Greydanus et al.](https://arxiv.org/abs/2006.08656)
:::

## Background: Coordinates matter

<!-- {{<video figures/coordinates.mp4 width="500" height="400" autoplay="true" loop="true">}} -->
<div>
<video width="700" height="500" loop style="display: block; margin-left: auto; margin-right: auto;">
  <source src="figures/coordinates.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</div>
Neural networks are coordinate dependent. The choice of coordinates can significantly affect the performance of the network.

::: {.attribution}
Adapted from Heliocentrism and Geocentrism by [Malin Christersson
](https://www.malinc.se/math/)
:::


# Metalearning Activation functions 
(Published and Patented)
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script><div class="altmetric-embed" data-badge-type="large-donut" data-altmetric-id="126463806"></div>

## Foundation {.center}

- Most complex systems showcase diversity in populations.
- Artificial neural network architectures are traditionally layerwise-homogenous due to computational efficiencies.
- Will neurons diversify if given the opportunity?



## Insight
:::: {layout="[ 65, 35 ]"}

::: {#first-column}
::: {.center-xy}
- Activation functions can themselves be modeled as neural networks.
- Through metalearning, we can optimize the activation function subnetwork for a given task.
- Using multiple neural network initializations, we can allow the activations to _diversify_ and evolve into different communities.
:::
:::

::: {#second-column}
![](figures/meta_act_schema_vert.png){fig-alt="metalearning" fig-align="center" width=80%}
:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Methodology
:::: {layout="[ 65, 35 ]"}

::: {#first-column}

- Developed a metalearning framework to optimize activation functions.
- Tested the algorithm on classification and regression tasks for conventional and physics-informed neural networks.
- Showed a regime where learned diverse activations are superior.
- Gave preliminary analysis to support the hypothesis that diversity in activation functions can improve performance.

:::

::: {#second-column}

![](figures/2NeuronSubpopulation_Sine_1_compact2.png){fig-alt="2neuronmnist1d" fig-align="center" width=100%}

:::
::::

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Results
![](figures/meta_div_scaling.png){fig-alt="metalearning" fig-align="center" width=80%}

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Results Real World Data
![](figures/real_pendulum.png){fig-alt="real world example" fig-align="center" width=80%}

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

## Analysis: Participation Ratio
Estimate change in dimensionality of network activations

:::: {layout="[ 65, 35 ]"}

::: {#first-column}

$$
Nr = \mathcal{R} 
    = \frac{(\operatorname{tr}{\bf C})^2}{\operatorname{tr}{\bf C}^2}
    = \frac{\left(\sum_{n=1}^N\lambda_n \right)^2}{\sum_{n=1}^N \lambda_n^2}
$$

where $\lambda_n$ are the co-variance matrix $\bf C$'s eigenvalues for neuronal activity data matrix.
The normalized participation ratio $r = \mathcal{R} / N$.

:::

::: {#second-column}

![](figures/Participation_ratio.png){fig-alt="participation ratio" fig-align="center" width=100%}

:::
::::

We find that the participation ratio is significantly greater for the diverse activation functions indicating a greater use of the network's capacity.

::: {.attribution}
Figure from [Publication](https://www.nature.com/articles/s41598-023-40766-6)
:::

::: {.notes}
If all the variance is in one dimension, say $\lambda_n = \delta_{n1}$, then $\mathcal{R} = 1$

If the variance is evenly distributed across all dimensions, so $\lambda_n = \lambda_1$, then $\mathcal{R} = N$. 

Typically, $1 < \mathcal{R} < N$, and $\mathcal{R}$ corresponds to the number of dimensions needed to explain most of the variance
:::

## Conclusions

- Learned Diversity Networks discover sets of activation functions that can outperform traditional activations.
- These networks excel in the regime of low data few shot learning.
- Due to the nature of metalearning, the memory overhead is significant concern for scaling.

## Speculations: Achieving stable minima
Optimization of a neural network with shuffled data is akin to noisy descent in the loss landscape.
This can me modeled with the Langevin equation:
$$
\DeclareMathOperator{\Tr}{Tr}
d\theta_{t} = - \nabla \mathcal{L}(\theta_{t})\, dt + \sqrt{2\mathbf{D}} \cdot d\mathcal{W}_{t}
$$
with noise intensity $\mathbf{D} = (\eta / B) \mathcal{L}(\theta) \mathbf{H}(\theta^*)$

Resulting minima from diverse neurons is flatter than from homogeneous ones, as measured by both the trace $\Tr\mathbf{H}$ of the Hessian and the fraction $f$ of its eigenvalues near zero: $\Tr \mathbf{H}_1 >  \Tr \mathbf{H}_2 > \Tr \mathbf{H}_{12}$ and $f_1 < f_2 < f_{12}$

Since the noise term aligns with Hessian bear the minima, this suggests that the minima found by diverse neurons are flatter and more stable.

## Speculations: Structure and universality to diverse activations

- Learned activation functions appear qualitatively independent of the base activation function.
- The odd and even nature of the learned functions suggest that the network is learning to span the space of possible activation functions.

![](figures/learned_act_insight.png){fig-alt="spanning sets" fig-align="center" width=80%}



# Background Deux

## Background: Pontryagin's Maximum Principle

## Background: Model Predictive Control

## Background: Chaos

## Background: Chaos Control

## Background: Pendulum
The career of a young theoretical physicist consists of treating the harmonic oscillator in ever-increasing levels of abstraction. - Sidney Coleman

## Background: Kuramoto Oscillator

## Background: Array Systems

## Background: Stochasticity and noise

# Neural Network control of chaos
## Foundation

## Insight

## Methodology

## Results (Preliminary)

## Future Work

## Speculation

# Background (For the last time I promise)
## Background: Symmetries

## Background: Dynamical Systems

## Background: Controllers for Dynamical systems

## Background: Group Equivariant Neural Networks

# Symmetries of controllers
## Foundation

## Insight

## Methodology

## Expectation

# Conclusions

## Conclusion

## Deliverables

<!-- 


## Code Highlighting

For continuous highlighting, use `from-to` (`6-8`).

For discontinuous highlighting, use `line1, line2, ...` (`1, 4`).

To highlight lines in a progressive manner, use `range1|range2` (`|6-8|1,4|`). 

```{.python code-line-numbers="|6-8|1,4|"}
import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 2 * np.pi * r
fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
```

<https://quarto.org/docs/presentations/revealjs/#code-blocks>

## Enable more revealjs features

The theme is built ontop of Quarto's `revealjs` implementation. So, any [features](https://quarto.org/docs/presentations/revealjs/) of available are also usable within the theme. For example, if we wanted to incorporate the [chalkboard](https://quarto.org/docs/presentations/revealjs/presenting.html#chalkboard) feature. We would use:

```yaml
format:
  ncsu-revealjs: 
    chalkboard: true
```

## Summary {#sec-summary}

### NCSU-themed presentation slide deck

The Quarto NCSU Revealjs theme is an extension of Reveal.js and
offers all of its [features](https://quarto.org/docs/presentations/revealjs/) in the context of being brand friendly to North Carolina State University.

Install the theme without this template:

```bash
quarto install extension ParticularlyPythonicBS/ncsu-revealjs
```

Install the theme with the template being present:

```bash
quarto use template ParticularlyPythonicBS/ncsu-revealjs
```

You can learn more about using RevealJS with Quarto at: <https://quarto.org/docs/presentations/revealjs/> -->