
@article{vicentini_netket_2022,
	title = {{NetKet} 3: {Machine} {Learning} {Toolbox} for {Many}-{Body} {Quantum} {Systems}},
	issn = {2949-804X},
	shorttitle = {{NetKet} 3},
	url = {https://scipost.org/10.21468/SciPostPhysCodeb.7},
	doi = {10.21468/SciPostPhysCodeb.7},
	abstract = {SciPost Journals Publication Detail SciPost Phys. Codebases 7 (2022) NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems},
	language = {en},
	urldate = {2024-04-22},
	journal = {SciPost Physics Codebases},
	author = {Vicentini, Filippo and Hofmann, Damian and Szabó, Attila and Wu, Dian and Roth, Christopher and Giuliani, Clemens and Pescia, Gabriel and Nys, Jannes and Vargas-Calderón, Vladimir and Astrakhantsev, Nikita and Carleo, Giuseppe},
	month = aug,
	year = {2022},
	pages = {007},
}

@misc{sohl-dickstein_boundary_2024,
	title = {The boundary of neural network trainability is fractal},
	url = {http://arxiv.org/abs/2402.06184},
	doi = {10.48550/arXiv.2402.06184},
	abstract = {Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06184 [nlin]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Nonlinear Sciences - Chaotic Dynamics},
}

@misc{blondel_elements_2024,
	title = {The {Elements} of {Differentiable} {Programming}},
	url = {http://arxiv.org/abs/2403.14606},
	doi = {10.48550/arXiv.2403.14606},
	abstract = {Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Blondel, Mathieu and Roulet, Vincent},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14606 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
}

@misc{greydanus_hamiltonian_2019,
	title = {Hamiltonian {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.01563},
	doi = {10.48550/arXiv.1906.01563},
	abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
	month = sep,
	year = {2019},
	note = {arXiv:1906.01563 [cs]
version: 3},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{choudhary_neuronal_2023,
	title = {Neuronal diversity can improve machine learning for physics and beyond},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-40766-6},
	doi = {10.1038/s41598-023-40766-6},
	abstract = {Diversity conveys advantages in nature, yet homogeneous neurons typically comprise the layers of artificial neural networks. Here we construct neural networks from neurons that learn their own activation functions, quickly diversify, and subsequently outperform their homogeneous counterparts on image classification and nonlinear regression tasks. Sub-networks instantiate the neurons, which meta-learn especially efficient sets of nonlinear responses. Examples include conventional neural networks classifying digits and forecasting a van der Pol oscillator and physics-informed Hamiltonian neural networks learning Hénon–Heiles stellar orbits and the swing of a video recorded pendulum clock. Such learned diversity provides examples of dynamical systems selecting diversity over uniformity and elucidates the role of diversity in natural and artificial systems.},
	language = {en},
	number = {1},
	urldate = {2024-04-22},
	journal = {Scientific Reports},
	author = {Choudhary, Anshul and Radhakrishnan, Anil and Lindner, John F. and Sinha, Sudeshna and Ditto, William L.},
	month = aug,
	year = {2023},
	keywords = {Information theory and computation, Nonlinear phenomena},
	pages = {13962},
}

@misc{gao_theory_2017,
	title = {A theory of multineuronal dimensionality, dynamics and measurement},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/214262v2},
	doi = {10.1101/214262},
	abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
	language = {en},
	urldate = {2024-04-22},
	publisher = {bioRxiv},
	author = {Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
	month = nov,
	year = {2017},
}

@misc{mori_power-law_2022,
	title = {Power-law escape rate of {SGD}},
	url = {http://arxiv.org/abs/2105.09557},
	doi = {10.48550/arXiv.2105.09557},
	abstract = {Stochastic gradient descent (SGD) undergoes complicated multiplicative noise for the mean-square loss. We use this property of SGD noise to derive a stochastic differential equation (SDE) with simpler additive noise by performing a random time change. Using this formalism, we show that the log loss barrier \${\textbackslash}Delta{\textbackslash}log L={\textbackslash}log[L({\textbackslash}theta{\textasciicircum}s)/L({\textbackslash}theta{\textasciicircum}*)]\$ between a local minimum \${\textbackslash}theta{\textasciicircum}*\$ and a saddle \${\textbackslash}theta{\textasciicircum}s\$ determines the escape rate of SGD from the local minimum, contrary to the previous results borrowing from physics that the linear loss barrier \${\textbackslash}Delta L=L({\textbackslash}theta{\textasciicircum}s)-L({\textbackslash}theta{\textasciicircum}*)\$ decides the escape rate. Our escape-rate formula strongly depends on the typical magnitude \$h{\textasciicircum}*\$ and the number \$n\$ of the outlier eigenvalues of the Hessian. This result explains an empirical fact that SGD prefers flat minima with low effective dimensions, giving an insight into implicit biases of SGD.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
	month = jan,
	year = {2022},
	note = {arXiv:2105.09557 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
}

@article{fiedler_-mpc:_2023,
	title = {do-mpc: {Towards} {FAIR} nonlinear and robust model predictive control},
	volume = {140},
	issn = {0967-0661},
	shorttitle = {do-mpc},
	url = {https://www.sciencedirect.com/science/article/pii/S0967066123002459},
	doi = {10.1016/j.conengprac.2023.105676},
	abstract = {Over the last decades, model predictive control (MPC) has shown outstanding performance for control tasks from various domains. This performance has further improved in recent years with advanced MPC schemes for nonlinear systems under uncertainty including economic control objectives. These recent improvements often fail to bridge the gap between MPC researchers and control practitioners in academia and industry, where classical control approaches and traditional linear MPC still dominate most applications. This is despite the fact that advanced MPC controllers can lead to significant energy savings, yield improvements, safer operation and other benefits. In this work, we identify four main obstacles hindering the widespread adoption of advanced MPC methods. These are the unavailability of models, the challenges associated with deploying complex controllers on physical systems, the scarcity of rapid prototyping tools for advanced methods and the limited reproducibility and reusability of advanced MPC controllers and their results. We find that the FAIR principles (findable, accessible, interoperable, reusable) for scientific data-management and research software can play an important role in tackling these obstacles. Following these guidelines, we discuss FAIR solutions and present the open-source software do-mpc as a concrete implementation. The presented solutions include interoperability with neural network toolboxes to simplify nonlinear system identification, interoperability with the OPC UA communication protocol for deployment, and a reproducible data-sampling framework for transparent controller validation, system identification and approximate MPC. The potential of the proposed solutions is illustrated with several simulation studies.},
	urldate = {2024-04-22},
	journal = {Control Engineering Practice},
	author = {Fiedler, Felix and Karg, Benjamin and Lüken, Lukas and Brandner, Dean and Heinlein, Moritz and Brabender, Felix and Lucia, Sergio},
	month = nov,
	year = {2023},
	keywords = {Nonlinear model predictive control, Learning-based control, Robust control},
	pages = {105676},
}

@book{devaney_introduction_2003,
	address = {Boulder, Colo},
	edition = {2nd ed., paperback ed},
	series = {Studies in {Nonlinearity}},
	title = {An introduction to chaotic dynamical systems},
	isbn = {9780813340852},
	language = {eng},
	publisher = {Westview Press},
	author = {Devaney, Robert L.},
	year = {2003},
}

@article{ott_controlling_1990,
	title = {Controlling chaos},
	volume = {64},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.64.1196},
	doi = {10.1103/PhysRevLett.64.1196},
	abstract = {It is shown that one can convert a chaotic attractor to any one of a large number of possible attracting time-periodic motions by making only small time-dependent perturbations of an available system parameter. The method utilizes delay coordinate embedding, and so is applicable to experimental situations in which a priori analytical knowledge of the system dynamics is not available. Important issues include the length of the chaotic transient preceding the periodic motion, and the effect of noise. These are illustrated with a numerical example., This article appears in the following collection:},
	number = {11},
	urldate = {2024-04-22},
	journal = {Physical Review Letters},
	author = {Ott, Edward and Grebogi, Celso and Yorke, James A.},
	month = mar,
	year = {1990},
	pages = {1196--1199},
}

@article{braiman_taming_1995,
	title = {Taming spatiotemporal chaos with disorder},
	volume = {378},
	copyright = {1995 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/378465a0},
	doi = {10.1038/378465a0},
	abstract = {DISORDERand noise in physical systems usually tend to destroy spatial and temporal regularity, but recent research into nonlinear systems provides intriguing counterexamples. In the phenomenon of stochastic resonance1, for example, the presence of noise improves the ability of some nonlinear systems to transfer information reliably. Noise can also remove chaos in a model oscillator2, and facilitate synchronization in an extended array of bistable elements3. Here we explore the use of disorder as a means to control spatiotemporal chaos4–8 in coupled arrays of forced, damped, nonlinear oscillators. Chaotic behaviour in spatially extended systems, especially in biology and physiology9,10, might be amenable to control, as occurs in lowdimensional temporally chaotic systems11,12. In our numerical experiments, one and twodimensional arrays of identical oscillators behave chaotically, but the introduction of slight, uncorrelated differences between the oscillators induces ordered motion characterized by complex but regular spatiotemporal patterns.},
	language = {en},
	number = {6556},
	urldate = {2024-04-22},
	journal = {Nature},
	author = {Braiman, Y. and Lindner, John F. and Ditto, William L.},
	month = nov,
	year = {1995},
	keywords = {Science, Humanities and Social Sciences, multidisciplinary, Science, multidisciplinary},
	pages = {465--467},
}

@article{ditto_experimental_1990,
	title = {Experimental control of chaos},
	volume = {65},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.65.3211},
	doi = {10.1103/PhysRevLett.65.3211},
	abstract = {We have achieved control of chaos in a physical system using the method of Ott, Grebogi, and Yorke [Phys. Rev. Lett. 64, 1196 (1990)]. The method requires only small time-dependent perturbations of a single-system parameter and does not require that one have model equations for the dynamics. We demonstrate the power of the method by controlling a chaotic system around unstable periodic orbits of order 1 and 2, switching between them at will.},
	number = {26},
	urldate = {2024-04-22},
	journal = {Physical Review Letters},
	author = {Ditto, W. L. and Rauseo, S. N. and Spano, M. L.},
	month = dec,
	year = {1990},
	pages = {3211--3214},
}

@inproceedings{kuramoto_international_1975,
	title = {In {International} {Symposium} on {Mathematical} {Problems} in {Theoretical} {Physics}},
	url = {https://api.semanticscholar.org/CorpusID:53832482},
	author = {Kuramoto, Yoshiki},
	year = {1975},
}

@article{bottcher_ai_2022,
	title = {{AI} {Pontryagin} or how artificial neural networks learn to control dynamical systems},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-27590-0},
	doi = {10.1038/s41467-021-27590-0},
	abstract = {The efficient control of complex dynamical systems has many applications in the natural and applied sciences. In most real-world control problems, both control energy and cost constraints play a significant role. Although such optimal control problems can be formulated within the framework of variational calculus, their solution for complex systems is often analytically and computationally intractable. To overcome this outstanding challenge, we present AI Pontryagin, a versatile control framework based on neural ordinary differential equations that automatically learns control signals that steer high-dimensional dynamical systems towards a desired target state within a specified time interval. We demonstrate the ability of AI Pontryagin to learn control signals that closely resemble those found by corresponding optimal control frameworks in terms of control energy and deviation from the desired target state. Our results suggest that AI Pontryagin is capable of solving a wide range of control and optimization problems, including those that are analytically intractable.},
	language = {en},
	number = {1},
	urldate = {2024-04-22},
	journal = {Nature Communications},
	author = {Böttcher, Lucas and Antulov-Fantulin, Nino and Asikis, Thomas},
	month = jan,
	year = {2022},
	keywords = {Complex networks, Computational science, Nonlinear phenomena},
	pages = {333},
}

@article{abrams_chimera_2004,
	title = {Chimera {States} for {Coupled} {Oscillators}},
	volume = {93},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/nlin/0407045},
	doi = {10.1103/PhysRevLett.93.174102},
	abstract = {Arrays of identical oscillators can display a remarkable spatiotemporal pattern in which phase-locked oscillators coexist with drifting ones. Discovered two years ago, such "chimera states" are believed to be impossible for locally or globally coupled systems; they are peculiar to the intermediate case of nonlocal coupling. Here we present an exact solution for this state, for a ring of phase oscillators coupled by a cosine kernel. We show that the stable chimera state bifurcates from a spatially modulated drift state, and dies in a saddle-node bifurcation with an unstable chimera.},
	number = {17},
	urldate = {2024-04-22},
	journal = {Physical Review Letters},
	author = {Abrams, Daniel M. and Strogatz, Steven H.},
	month = oct,
	year = {2004},
	note = {arXiv:nlin/0407045},
	keywords = {Nonlinear Sciences - Pattern Formation and Solitons},
	pages = {174102},
}

@article{kennel_determining_1992,
	title = {Determining embedding dimension for phase-space reconstruction using a geometrical construction},
	volume = {45},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.45.3403},
	doi = {10.1103/PhysRevA.45.3403},
	abstract = {We examine the issue of determining an acceptable minimum embedding dimension by looking at the behavior of near neighbors under changes in the embedding dimension from d→d+1. When the number of nearest neighbors arising through projection is zero in dimension dE, the attractor has been unfolded in this dimension. The precise determination of dE is clouded by ‘‘noise,’’ and we examine the manner in which noise changes the determination of dE. Our criterion also indicates the error one makes by choosing an embedding dimension smaller than dE. This knowledge may be useful in the practical analysis of observed time series.},
	number = {6},
	urldate = {2024-04-22},
	journal = {Physical Review A},
	author = {Kennel, Matthew B. and Brown, Reggie and Abarbanel, Henry D. I.},
	month = mar,
	year = {1992},
	pages = {3403--3411},
}

@book{zee_group_2016,
	address = {Princeton},
	series = {In a nutshell},
	title = {Group theory in a nutshell for physicists},
	isbn = {9780691162690},
	publisher = {Princeton University Press},
	author = {Zee, A.},
	year = {2016},
	keywords = {Group theory},
}

@article{smets_pde-based_2023,
	title = {{PDE}-based {Group} {Equivariant} {Convolutional} {Neural} {Networks}},
	volume = {65},
	issn = {0924-9907, 1573-7683},
	url = {http://arxiv.org/abs/2001.09046},
	doi = {10.1007/s10851-022-01114-x},
	abstract = {We present a PDE-based framework that generalizes Group equivariant Convolutional Neural Networks (G-CNNs). In this framework, a network layer is seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients become the layer's trainable weights. Formulating our PDEs on homogeneous spaces allows these networks to be designed with built-in symmetries such as rotation in addition to the standard translation equivariance of CNNs. Having all the desired symmetries included in the design obviates the need to include them by means of costly techniques such as data augmentation. We will discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space setting while also going into the specifics of our primary case of interest: roto-translation equivariance. We solve the PDE of interest by a combination of linear group convolutions and non-linear morphological group convolutions with analytic kernel approximations that we underpin with formal theorems. Our kernel approximations allow for fast GPU-implementation of the PDE-solvers, we release our implementation with this article in the form of the LieTorch extension to PyTorch, available at https://gitlab.com/bsmetsjr/lietorch . Just like for linear convolution a morphological convolution is specified by a kernel that we train in our PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling and ReLUs as they are already subsumed by morphological convolutions. We present a set of experiments to demonstrate the strength of the proposed PDE-G-CNNs in increasing the performance of deep learning based imaging applications with far fewer parameters than traditional CNNs.},
	number = {1},
	urldate = {2024-04-22},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Smets, Bart and Portegies, Jim and Bekkers, Erik and Duits, Remco},
	month = jan,
	year = {2023},
	note = {arXiv:2001.09046 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Mathematics - Differential Geometry, Statistics - Machine Learning},
	pages = {209--239},
}

@inproceedings{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {https://proceedings.mlr.press/v48/cohenc16.html},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
	language = {en},
	urldate = {2024-04-22},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cohen, Taco and Welling, Max},
	month = jun,
	year = {2016},
	pages = {2990--2999},
}

@article{leshno_multilayer_1993,
	title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
	volume = {6},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
	doi = {10.1016/S0893-6080(05)80131-5},
	abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.},
	number = {6},
	urldate = {2024-04-22},
	journal = {Neural Networks},
	author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
	month = jan,
	year = {1993},
	keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
	pages = {861--867},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2024-04-22},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Neural networks, Approximation, Completeness},
	pages = {303--314},
}
